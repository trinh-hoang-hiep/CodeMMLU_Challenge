{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip freeze  ##https://fsoft-ai4code.github.io/leaderboards/codemmlu/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWiyyU8fDoll",
        "outputId": "9a161e78-30b0-4529-85a2-1c86bac24ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==1.5.2\n",
            "aiohappyeyeballs==2.6.1\n",
            "aiohttp==3.11.14\n",
            "aiosignal==1.3.2\n",
            "alabaster==1.0.0\n",
            "albucore==0.0.23\n",
            "albumentations==2.0.5\n",
            "ale-py==0.10.2\n",
            "altair==5.5.0\n",
            "annotated-types==0.7.0\n",
            "anyio==4.9.0\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.7.1\n",
            "arviz==0.21.0\n",
            "astropy==7.0.1\n",
            "astropy-iers-data==0.2025.3.17.0.34.53\n",
            "astunparse==1.6.3\n",
            "atpublic==5.1\n",
            "attrs==25.3.0\n",
            "audioread==3.0.1\n",
            "autograd==1.7.0\n",
            "babel==2.17.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.13.3\n",
            "betterproto==2.0.0b6\n",
            "bigframes==1.41.0\n",
            "bigquery-magics==0.8.0\n",
            "bleach==6.2.0\n",
            "blinker==1.9.0\n",
            "blis==1.2.0\n",
            "blosc2==3.2.0\n",
            "bokeh==3.6.3\n",
            "Bottleneck==1.4.2\n",
            "bqplot==0.12.44\n",
            "branca==0.8.1\n",
            "CacheControl==0.14.2\n",
            "cachetools==5.5.2\n",
            "catalogue==2.0.10\n",
            "certifi==2025.1.31\n",
            "cffi==1.17.1\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.4.1\n",
            "chex==0.1.89\n",
            "clarabel==0.10.0\n",
            "click==8.1.8\n",
            "cloudpathlib==0.21.0\n",
            "cloudpickle==3.1.1\n",
            "cmake==3.31.6\n",
            "cmdstanpy==1.2.5\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contourpy==1.3.1\n",
            "cramjam==2.9.1\n",
            "cryptography==43.0.3\n",
            "cuda-python==12.6.2.post1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "cudf-polars-cu12==25.2.2\n",
            "cufflinks==0.17.3\n",
            "cuml-cu12==25.2.1\n",
            "cupy-cuda12x==13.3.0\n",
            "cuvs-cu12==25.2.1\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.6.4\n",
            "cycler==0.12.1\n",
            "cyipopt==1.5.0\n",
            "cymem==2.0.11\n",
            "Cython==3.0.12\n",
            "dask==2024.12.1\n",
            "dask-cuda==25.2.0\n",
            "dask-cudf-cu12==25.2.2\n",
            "dask-expr==1.1.21\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.4.2\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.8.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "Deprecated==1.2.18\n",
            "diffusers==0.32.2\n",
            "distributed==2024.12.1\n",
            "distributed-ucxx-cu12==0.42.0\n",
            "distro==1.9.0\n",
            "dlib==19.24.2\n",
            "dm-tree==0.1.9\n",
            "docker-pycreds==0.4.0\n",
            "docstring_parser==0.16\n",
            "docutils==0.21.2\n",
            "dopamine_rl==4.1.2\n",
            "duckdb==1.2.1\n",
            "earthengine-api==1.5.7\n",
            "easydict==1.13\n",
            "editdistance==0.8.1\n",
            "eerepr==0.1.1\n",
            "einops==0.8.1\n",
            "en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85\n",
            "entrypoints==0.4\n",
            "et_xmlfile==2.0.0\n",
            "etils==1.12.2\n",
            "etuples==0.3.9\n",
            "Farama-Notifications==0.0.4\n",
            "fastai==2.7.19\n",
            "fastcore==1.7.29\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.21.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.3\n",
            "filelock==3.18.0\n",
            "firebase-admin==6.7.0\n",
            "Flask==3.1.0\n",
            "flatbuffers==25.2.10\n",
            "flax==0.10.4\n",
            "folium==0.19.5\n",
            "fonttools==4.56.0\n",
            "frozendict==2.4.6\n",
            "frozenlist==1.5.0\n",
            "fsspec==2025.3.0\n",
            "future==1.0.0\n",
            "gast==0.6.0\n",
            "gcsfs==2025.3.0\n",
            "GDAL==3.6.4\n",
            "gdown==5.2.0\n",
            "geemap==0.35.3\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==1.0.1\n",
            "geopy==2.4.1\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.12\n",
            "GitPython==3.1.44\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.15\n",
            "google-api-core==2.24.2\n",
            "google-api-python-client==2.164.0\n",
            "google-auth==2.38.0\n",
            "google-auth-httplib2==0.2.0\n",
            "google-auth-oauthlib==1.2.1\n",
            "google-cloud-aiplatform==1.84.0\n",
            "google-cloud-bigquery==3.29.0\n",
            "google-cloud-bigquery-connection==1.18.2\n",
            "google-cloud-bigquery-storage==2.29.1\n",
            "google-cloud-bigtable==2.30.0\n",
            "google-cloud-core==2.4.3\n",
            "google-cloud-dataproc==5.18.1\n",
            "google-cloud-datastore==2.20.2\n",
            "google-cloud-firestore==2.20.1\n",
            "google-cloud-functions==1.20.2\n",
            "google-cloud-iam==2.18.3\n",
            "google-cloud-language==2.17.1\n",
            "google-cloud-pubsub==2.29.0\n",
            "google-cloud-resource-manager==1.14.2\n",
            "google-cloud-spanner==3.53.0\n",
            "google-cloud-storage==2.19.0\n",
            "google-cloud-translate==3.20.2\n",
            "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
            "google-crc32c==1.7.0\n",
            "google-genai==1.7.0\n",
            "google-generativeai==0.8.4\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.2\n",
            "google-spark-connect==0.5.2\n",
            "googleapis-common-protos==1.69.2\n",
            "googledrivedownloader==1.1.0\n",
            "graphviz==0.20.3\n",
            "greenlet==3.1.1\n",
            "grpc-google-iam-v1==0.14.2\n",
            "grpc-interceptor==0.15.4\n",
            "grpcio==1.71.0\n",
            "grpcio-status==1.71.0\n",
            "grpclib==0.4.7\n",
            "gspread==6.2.0\n",
            "gspread-dataframe==4.0.0\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "gymnasium==1.1.1\n",
            "h11==0.14.0\n",
            "h2==4.2.0\n",
            "h5netcdf==1.6.1\n",
            "h5py==3.13.0\n",
            "hdbscan==0.8.40\n",
            "highspy==1.9.0\n",
            "holidays==0.69\n",
            "holoviews==1.20.2\n",
            "hpack==4.1.0\n",
            "html5lib==1.1\n",
            "httpcore==1.0.7\n",
            "httpimport==1.4.1\n",
            "httplib2==0.22.0\n",
            "httpx==0.28.1\n",
            "huggingface-hub==0.29.3\n",
            "humanize==4.12.1\n",
            "hyperframe==6.1.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==9.5.0\n",
            "idna==3.10\n",
            "imageio==2.37.0\n",
            "imageio-ffmpeg==0.6.0\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.13.0\n",
            "immutabledict==4.2.1\n",
            "importlib_metadata==8.6.1\n",
            "importlib_resources==6.5.2\n",
            "imutils==0.5.4\n",
            "inflect==7.5.0\n",
            "iniconfig==2.1.0\n",
            "intel-cmplr-lib-ur==2025.1.0\n",
            "intel-openmp==2025.1.0\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==6.17.1\n",
            "ipyleaflet==0.19.2\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jax==0.5.2\n",
            "jax-cuda12-pjrt==0.5.1\n",
            "jax-cuda12-plugin==0.5.1\n",
            "jaxlib==0.5.1\n",
            "jeepney==0.7.1\n",
            "jellyfish==1.1.0\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.6\n",
            "jiter==0.9.0\n",
            "joblib==1.4.2\n",
            "jsonpatch==1.33\n",
            "jsonpickle==4.0.2\n",
            "jsonpointer==3.0.0\n",
            "jsonschema==4.23.0\n",
            "jsonschema-specifications==2024.10.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-leaflet==0.19.2\n",
            "jupyter-server==1.16.0\n",
            "jupyter_core==5.7.2\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.13\n",
            "kaggle==1.7.4.2\n",
            "kagglehub==0.3.10\n",
            "keras==3.8.0\n",
            "keras-hub==0.18.1\n",
            "keras-nlp==0.18.1\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.8\n",
            "langchain==0.3.21\n",
            "langchain-core==0.3.47\n",
            "langchain-text-splitters==0.3.7\n",
            "langcodes==3.5.0\n",
            "langsmith==0.3.18\n",
            "language_data==1.3.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl\n",
            "libcugraph-cu12==25.2.0\n",
            "libcuml-cu12==25.2.1\n",
            "libcuvs-cu12==25.2.1\n",
            "libkvikio-cu12==25.2.1\n",
            "libraft-cu12==25.2.0\n",
            "librosa==0.11.0\n",
            "libucx-cu12==1.18.0\n",
            "libucxx-cu12==0.42.0\n",
            "lightgbm==4.5.0\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.43.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==5.3.1\n",
            "Mako==1.1.3\n",
            "marisa-trie==1.2.1\n",
            "Markdown==3.7\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==3.0.2\n",
            "matplotlib==3.10.0\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==1.1.2\n",
            "mdit-py-plugins==0.4.2\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==3.1.3\n",
            "mizani==0.13.1\n",
            "mkl==2025.0.1\n",
            "ml-dtypes==0.4.1\n",
            "mlxtend==0.23.4\n",
            "more-itertools==10.6.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.1.0\n",
            "multidict==6.2.0\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.12\n",
            "music21==9.3.0\n",
            "namex==0.0.8\n",
            "narwhals==1.31.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.2.0\n",
            "nbclient==0.10.2\n",
            "nbconvert==7.16.6\n",
            "nbformat==5.10.4\n",
            "ndindex==1.9.2\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.4.2\n",
            "nibabel==5.3.2\n",
            "nltk==3.9.1\n",
            "notebook==6.5.7\n",
            "notebook_shim==0.2.4\n",
            "numba==0.60.0\n",
            "numba-cuda==0.2.0\n",
            "numexpr==2.10.2\n",
            "numpy==2.0.2\n",
            "nvidia-cublas-cu12==12.5.3.2\n",
            "nvidia-cuda-cupti-cu12==12.5.82\n",
            "nvidia-cuda-nvcc-cu12==12.5.82\n",
            "nvidia-cuda-nvrtc-cu12==12.5.82\n",
            "nvidia-cuda-runtime-cu12==12.5.82\n",
            "nvidia-cudnn-cu12==9.3.0.75\n",
            "nvidia-cufft-cu12==11.2.3.61\n",
            "nvidia-curand-cu12==10.3.6.82\n",
            "nvidia-cusolver-cu12==11.6.3.83\n",
            "nvidia-cusparse-cu12==12.5.1.3\n",
            "nvidia-cusparselt-cu12==0.6.2\n",
            "nvidia-ml-py==12.570.86\n",
            "nvidia-nccl-cu12==2.21.5\n",
            "nvidia-nvcomp-cu12==4.2.0.11\n",
            "nvidia-nvjitlink-cu12==12.5.82\n",
            "nvidia-nvtx-cu12==12.4.127\n",
            "nvtx==0.2.11\n",
            "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "openai==1.68.2\n",
            "opencv-contrib-python==4.11.0.86\n",
            "opencv-python==4.11.0.86\n",
            "opencv-python-headless==4.11.0.86\n",
            "openpyxl==3.1.5\n",
            "opentelemetry-api==1.31.1\n",
            "opentelemetry-sdk==1.31.1\n",
            "opentelemetry-semantic-conventions==0.52b1\n",
            "opt_einsum==3.4.0\n",
            "optax==0.2.4\n",
            "optree==0.14.1\n",
            "orbax-checkpoint==0.11.10\n",
            "orjson==3.10.15\n",
            "osqp==0.6.7.post3\n",
            "packaging==24.2\n",
            "pandas==2.2.2\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.28.0\n",
            "pandas-stubs==2.2.2.240909\n",
            "pandocfilters==1.5.1\n",
            "panel==1.6.1\n",
            "param==2.2.0\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==1.0.1\n",
            "peewee==3.17.9\n",
            "peft==0.14.0\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "pillow==11.1.0\n",
            "platformdirs==4.3.7\n",
            "plotly==5.24.1\n",
            "plotnine==0.14.5\n",
            "pluggy==1.5.0\n",
            "ply==3.11\n",
            "polars==1.21.0\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "preshed==3.0.9\n",
            "prettytable==3.15.1\n",
            "proglog==0.1.10\n",
            "progressbar2==4.5.0\n",
            "prometheus_client==0.21.1\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.50\n",
            "propcache==0.3.0\n",
            "prophet==1.1.6\n",
            "proto-plus==1.26.1\n",
            "protobuf==5.29.4\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.10\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==18.1.0\n",
            "pyasn1==0.6.1\n",
            "pyasn1_modules==0.4.1\n",
            "pycairo==1.27.0\n",
            "pycocotools==2.0.8\n",
            "pycparser==2.22\n",
            "pydantic==2.10.6\n",
            "pydantic_core==2.27.2\n",
            "pydata-google-auth==1.9.1\n",
            "pydot==3.0.4\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.21.3\n",
            "pyerfa==2.0.1.5\n",
            "pygame==2.6.1\n",
            "pygit2==1.17.0\n",
            "Pygments==2.18.0\n",
            "PyGObject==3.42.0\n",
            "PyJWT==2.10.1\n",
            "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "pylibcugraph-cu12==25.2.0\n",
            "pylibraft-cu12==25.2.0\n",
            "pymc==5.21.1\n",
            "pymystem3==0.2.0\n",
            "pynndescent==0.5.13\n",
            "pynvjitlink-cu12==0.5.2\n",
            "pynvml==12.0.0\n",
            "pyogrio==0.10.0\n",
            "Pyomo==6.8.2\n",
            "PyOpenGL==3.1.9\n",
            "pyOpenSSL==24.2.1\n",
            "pyparsing==3.2.1\n",
            "pyperclip==1.9.0\n",
            "pyproj==3.7.1\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pyspark==3.5.5\n",
            "pytensor==2.28.3\n",
            "pytest==8.3.5\n",
            "python-apt==0.0.0\n",
            "python-box==7.3.2\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.4\n",
            "python-snappy==0.7.3\n",
            "python-utils==3.9.1\n",
            "pytz==2025.1\n",
            "pyviz_comms==3.0.4\n",
            "PyYAML==6.0.2\n",
            "pyzmq==24.0.1\n",
            "qdldl==0.1.7.post5\n",
            "raft-dask-cu12==25.2.0\n",
            "rapids-dask-dependency==25.2.0\n",
            "ratelim==0.1.6\n",
            "referencing==0.36.2\n",
            "regex==2024.11.6\n",
            "requests==2.32.3\n",
            "requests-oauthlib==2.0.0\n",
            "requests-toolbelt==1.0.0\n",
            "requirements-parser==0.9.0\n",
            "rich==13.9.4\n",
            "rmm-cu12==25.2.0\n",
            "roman-numerals-py==3.1.0\n",
            "rpds-py==0.23.1\n",
            "rpy2==3.5.17\n",
            "rsa==4.9\n",
            "safetensors==0.5.3\n",
            "scikit-image==0.25.2\n",
            "scikit-learn==1.6.1\n",
            "scipy==1.14.1\n",
            "scooby==0.10.0\n",
            "scs==3.2.7.post2\n",
            "seaborn==0.13.2\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.3\n",
            "sentence-transformers==3.4.1\n",
            "sentencepiece==0.2.0\n",
            "sentry-sdk==2.24.0\n",
            "setproctitle==1.3.5\n",
            "shap==0.47.0\n",
            "shapely==2.0.7\n",
            "shellingham==1.5.4\n",
            "simple-parsing==0.1.7\n",
            "simplejson==3.20.1\n",
            "simsimd==6.2.1\n",
            "six==1.17.0\n",
            "sklearn-compat==0.1.3\n",
            "sklearn-pandas==2.2.0\n",
            "slicer==0.0.8\n",
            "smart-open==7.1.0\n",
            "smmap==5.0.2\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.13.1\n",
            "soupsieve==2.6\n",
            "soxr==0.5.0.post1\n",
            "spacy==3.8.4\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "spanner-graph-notebook==1.1.5\n",
            "Sphinx==8.2.3\n",
            "sphinxcontrib-applehelp==2.0.0\n",
            "sphinxcontrib-devhelp==2.0.0\n",
            "sphinxcontrib-htmlhelp==2.1.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==2.0.0\n",
            "sphinxcontrib-serializinghtml==2.0.0\n",
            "SQLAlchemy==2.0.39\n",
            "sqlglot==25.20.2\n",
            "sqlparse==0.5.3\n",
            "srsly==2.5.1\n",
            "stanio==0.5.1\n",
            "statsmodels==0.14.4\n",
            "stringzilla==3.12.3\n",
            "sympy==1.13.1\n",
            "tables==3.10.2\n",
            "tabulate==0.9.0\n",
            "tbb==2022.1.0\n",
            "tblib==3.0.0\n",
            "tcmlib==1.3.0\n",
            "tenacity==9.0.0\n",
            "tensorboard==2.18.0\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.18.0\n",
            "tensorflow-datasets==4.9.8\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.16.1\n",
            "tensorflow-probability==0.25.0\n",
            "tensorflow-text==2.18.1\n",
            "tensorstore==0.1.72\n",
            "termcolor==2.5.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.19.0\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.18.0\n",
            "thinc==8.3.4\n",
            "threadpoolctl==3.6.0\n",
            "tifffile==2025.3.13\n",
            "timm==1.0.15\n",
            "tinycss2==1.4.0\n",
            "tokenizers==0.21.1\n",
            "toml==0.10.2\n",
            "toolz==0.12.1\n",
            "torch @ https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "tornado==6.4.2\n",
            "tqdm==4.67.1\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.50.0\n",
            "treelite==4.4.1\n",
            "treescope==0.1.9\n",
            "triton==3.2.0\n",
            "tweepy==4.15.0\n",
            "typeguard==4.4.2\n",
            "typer==0.15.2\n",
            "types-pytz==2025.1.0.20250318\n",
            "types-setuptools==76.0.0.20250313\n",
            "typing_extensions==4.12.2\n",
            "tzdata==2025.1\n",
            "tzlocal==5.3.1\n",
            "uc-micro-py==1.0.3\n",
            "ucx-py-cu12==0.42.0\n",
            "ucxx-cu12==0.42.0\n",
            "umap-learn==0.5.7\n",
            "umf==0.10.0\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.3.0\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wandb==0.19.8\n",
            "wasabi==1.1.3\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.11.1\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "websockets==15.0.1\n",
            "Werkzeug==3.1.3\n",
            "widgetsnbextension==3.6.10\n",
            "wordcloud==1.9.4\n",
            "wrapt==1.17.2\n",
            "xarray==2025.1.2\n",
            "xarray-einstats==0.8.0\n",
            "xgboost==2.1.4\n",
            "xlrd==2.0.1\n",
            "xyzservices==2025.1.0\n",
            "yarl==1.18.3\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.55\n",
            "zict==3.0.0\n",
            "zipp==3.21.0\n",
            "zstandard==0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cec27b56-aa27-41b9-ba46-d311bf4c6324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.3.19-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.3.17 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.3.17-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.50.0)\n",
            "Collecting datasets>=2.16.0 (from unsloth)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\n",
            "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n",
            "Collecting protobuf<4.0.0 (from unsloth)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.29.3)\n",
            "Collecting hf_transfer (from unsloth)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Collecting xxhash (from datasets>=2.16.0->unsloth)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.3.17->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth) (11.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->unsloth)\n",
            "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.3.19-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.3.17-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.18-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, typing-extensions, shtab, protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf_transfer, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, nvidia-cusolver-cu12, datasets, xformers, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.4 cut_cross_entropy-25.1.1 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 hf_transfer-0.1.9 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 protobuf-3.20.3 shtab-1.7.1 trl-0.15.2 typing-extensions-4.13.0 tyro-0.9.18 unsloth-2025.3.19 unsloth_zoo-2025.3.17 xformers-0.0.29.post3 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ab0eb3c7a65d44d19a08384698f78ae1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: unsloth 2025.3.19\n",
            "Uninstalling unsloth-2025.3.19:\n",
            "  Successfully uninstalled unsloth-2025.3.19\n",
            "Collecting git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-nv2d21ic\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-nv2d21ic\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 2ff5dc1a8de1614994a275785b7b64fb4db8cb5d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2025.3.19-py3-none-any.whl size=192249 sha256=72547e162efb2e861b585dbb1e0a6ab10bea9c0199fb15f1986db02eb84df096\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d5vei9xd/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2025.3.19\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llama_cot_promt_A"
      ],
      "metadata": {
        "id": "l0aweLRTT1ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2548 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    device_map = {\"\": 0}, ####\"auto\",\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "P7FTZcEM41Sd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "8cfd10112c524186b5e6affae6084998",
            "40b57e951743436786fec3f669504732",
            "35593b6060ce4b33ac2ee03b9595f43c",
            "cf8842d289694058a2ff2f2deb7aeec8",
            "8c3ca9b29f394a1ab00dde6540b70d4a",
            "50b6d482585c483e92b85d35f60f35c0",
            "bbe5b3c74bf044d8a29bd491b22961f0",
            "8a226cc7788c42ad8b7f6e6de466fa5c",
            "0d5e6833219147c9995bf24b351308bb",
            "c40d3f5e077148d1ad2d6b66df096298",
            "eab46d56a03f435d931ead9448820357",
            "79fb284ac359436abd83556f61081320",
            "0b34592c34154477b03a144e2f86beaa",
            "cab5b5c531e3482aa606fe5ce47976aa",
            "b5f33998d8164dbe966d48a4e4c3b2d3",
            "2dc083de20ad4043a8eb4e1e9c861d54",
            "60a56983934b4f3d95bd7411d851bf64",
            "a5f69e8f53aa4e5fb0cde6cbcd2b5a1f",
            "8f17c70b77824645822d02298929b308",
            "be5688fce75a41c1b4652e8435c35c7d",
            "420daf1a24a241e38bea251d3dc5d76f",
            "be1fdc4da45f45aebaa4b39275b5f83f",
            "a235deb6b4ad41469d71adb663bdc75d",
            "a5dbc5190c5c4307862ec5b8e95d3b2d",
            "f6196cdb1f294c23867be2e0c3491c7b",
            "e31df3d3be254b669b48f4f40d9128a2",
            "b728a04e209448778ad56d60f22c280f",
            "b7263272c3f84995bf1ead854136c088",
            "4debddb59c9946d497ff562242abf389",
            "fe1493c5a6424e9d8228e987873e8df7",
            "7aa12b27fdcd4e038dd98ab82f1dbdc7",
            "a0f49eb9c4c647628b184a1d5bf81102",
            "4864216bccf74e9aafbfef27c8e93ff2",
            "1d7793b5e7dc4403837b7c9b762e9811",
            "a4fc1e24c2b14eaea946ae5c83a547bd",
            "2985d6bd22f34d1fa7b2df3a80e26f65",
            "26c134aaa07b41008094c3f5fc99860d",
            "8fc7c654f1054fec9067b6f9ae9dbaab",
            "7046b49bd5f340bc926a93ec4068a9f3",
            "0894698255114f518385aabb9b85cceb",
            "03524fe50f5348b6b886a88910e40cd2",
            "ac87a7dd9fcd48e498578124705164f8",
            "05cdb9c71dd34e3ba261a1bf8540dc90",
            "64f280fa3f364a148f27be633d4e8602",
            "261e4ff5645a4f1ca809b5dde7629560",
            "009a5e12304c4168af679fa6a991fe1f",
            "00597430a30d4015ba7453de03b0c352",
            "dd879f3b989f4a45874e392b6503bdcc",
            "2c77356ce1724002ade402c363952527",
            "713aba8a3041420c9ee310b4ddb75d0a",
            "4194e22e837e4c48a34b0c257452c4e5",
            "fb71ff03f13f41beb95da131268243be",
            "019937b69fd7426a992d4a666d7ac6cb",
            "8ed38353eafa4ef3a909de51eea4081e",
            "f40997ca82d74070b63668495fcae48f"
          ]
        },
        "outputId": "23a4adcf-0bda-48f1-db17-2b642ceb3202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cfd10112c524186b5e6affae6084998"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79fb284ac359436abd83556f61081320"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a235deb6b4ad41469d71adb663bdc75d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d7793b5e7dc4403837b7c9b762e9811"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "261e4ff5645a4f1ca809b5dde7629560"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "2XJVSYZ6UMIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de8f03e-92e1-4f56-d645-3b93430c2378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length=2548\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "task_instruction=\"You are an AI model that can answer coding-related multiple-choice programming-related questions as accurately as possible base on context (Includes the question after the 'Question:' section and the choices enclosed in square brackets are indexed alphabetically A B C D E F G H... There is only 1 correct answer and only 1 uppercase letter is allowed strictly) If you don't know  the answers choose A\"\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "candidatemapping={\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\":6, \"H\":7}\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/CodeMMLU_sample/b6_train_data.csv\")\n",
        "\n",
        "list_data=[]\n",
        "for index, row in data.iterrows():\n",
        "    if  str(row['answer']).split(\" \")[-1] in \"ABCDEFGH\":\n",
        "      list_data.append({'input': str(row['question']),'output': str(row['answer']).split(\" \")[-1]})\n",
        "dataset=Dataset.from_list(list_data)\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    # The error ArrowInvalid: Column 2 named text expected length 1000 but got length 122 arises because the formatting_prompts_func is likely designed to process datasets with a specific structure that the dataset variable does not conform to. This function may be expecting the dataset to contain 1000 rows, but it encounters only 122 in the input.\n",
        "    # Instructions: The original code was passing task_instruction (a single string) to the zip function, which expects iterables of the same length. I changed it to create a list of task_instruction repeated for the length of the examples['input'] list.\n",
        "    # Iterating: The formatting_prompts_func is called by the map method and receives a batch of examples. The function should be adapted to handle batched data instead of individual examples. This is done by replacing the original code with a loop that iterates over the batch of examples and applies the prompt template.\n",
        "    # By making these changes, we ensure that the formatting_prompts_func is applied correctly to each example in the dataset, avoiding the error.\n",
        "    instructions = [task_instruction] * len(examples['input'])\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    out_extra=[]#########################\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        # if len(text) < max_length:\n",
        "        #     text = text.ljust(max_length, ' ')\n",
        "        #     print(len(text))\n",
        "        texts.append(text)\n",
        "        out_extra.append(candidatemapping[output])\n",
        "    return { \"text\" : texts,\"task\":[1]* len(examples['input']) }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "2bW7tPHJUMRk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f53f35fecaba4d1f93d7e274917f07b7",
            "fde00f495216423b9e534ed5ba19259f",
            "e312c78649574f9bbbaa16d8e4a5f5ad",
            "766c595537064cbbb0cdb8188fc1e921",
            "6d1a26d7d559462196c340269d10c1a1",
            "7717926adfb14c5583bf3a1b5deb9cfc",
            "53a3999637f54beeab4c9a90d72797cc",
            "5c7ac5002cfe4bf782b2b205ab989bfd",
            "e3682571366346f1b64933e6edbc7773",
            "39403b56f6784e2db79d6144dc2eeb53",
            "fbe6c80b65ae4681946667d0ed5690b8"
          ]
        },
        "outputId": "e68c3101-312f-4211-cfe0-511e025fd304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3949 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f53f35fecaba4d1f93d7e274917f07b7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import TrainerCallback\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "class ChebyshevWeightDecayCallback(TrainerCallback):\n",
        "    def __init__(self, total_steps, coeffs):\n",
        "        \"\"\"\n",
        "        total_steps: tổng số bước (iterations) huấn luyện.\n",
        "        coeffs: danh sách các hệ số Chebyshev [c0, c1, c2, ...] cho đa thức Chebyshev T0, T1, T2,...\n",
        "        \"\"\"\n",
        "        self.total_steps = total_steps\n",
        "        self.coeffs = coeffs\n",
        "\n",
        "    def compute_weight_decay(self, current_step):\n",
        "        # Map current_step từ [0, total_steps] sang [-1, 1]\n",
        "        x = 2 * (current_step / self.total_steps) - 1\n",
        "        # Tính giá trị của chuỗi Chebyshev tại x:\n",
        "        # wd = c0*T0(x) + c1*T1(x) + c2*T2(x) + ...\n",
        "        wd = np.polynomial.chebyshev.chebval(x, self.coeffs)\n",
        "        return float(wd)\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        current_step = state.global_step\n",
        "        new_wd = self.compute_weight_decay(current_step)\n",
        "        optimizer = kwargs.get(\"optimizer\", None)\n",
        "        if optimizer is not None:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group[\"weight_decay\"] = new_wd\n",
        "        # In ra để debug\n",
        "        print(f\"Iteration {current_step}: weight_decay set to {new_wd:.6f}\")\n",
        "        return control\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer  # hoặc import từ nguồn tương ứng\n",
        "\n",
        "# Ví dụ: tổng số bước huấn luyện\n",
        "total_steps = 160\n",
        "# Ví dụ hệ số Chebyshev: các giá trị này bạn có thể tùy chỉnh theo nhu cầu\n",
        "cheb_coeffs = [0.01, 0.005, -0.002]  # Tương ứng với c0, c1, c2\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,  # Giúp training nhanh hơn nếu chuỗi ngắn.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # max_steps = total_steps,\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,  # Giá trị ban đầu, sẽ được cập nhật qua callback.\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",  # Nếu không dùng WandB, v.v.\n",
        "    ),\n",
        "    callbacks = [ChebyshevWeightDecayCallback(total_steps, cheb_coeffs)]\n",
        ")\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "h1iQpzOZUM4Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7d8749855f0e4383a4ae7fc8c0393992",
            "0e8a4c58c181496aadacda715ab8c4dd",
            "9df7309d6a8444588732d419ac9fdd69",
            "b9d10e5dce074081932b1c67986e551f",
            "23101f767e2244aebb8a16fca4f9d744",
            "425b55147263401498412119de507b2e",
            "8ca762aa59154886ac178f5174cae605",
            "9606651acd2149cdb87e664176d13a18",
            "3bfdda8a3e474516b7a5843571c65819",
            "ebf084a63a824cc5bdee0928f5fc6f21",
            "3e67e9c0fc0f44b295f4ffc9fb2cb908"
          ]
        },
        "outputId": "350fa133-8a6d-4290-955f-e5520b6eafb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/3949 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d8749855f0e4383a4ae7fc8c0393992"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,949 | Num Epochs = 1 | Total steps = 247\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "Iteration 1: weight_decay set to 0.003162\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='164' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [164/247 56:02 < 28:42, 0.05 it/s, Epoch 0.66/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.222800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.369900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.100500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.335000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.973700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.638300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.605300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.236500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.096400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.061900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.882400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.629800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.691600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.544600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.578900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.480700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.495700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.509000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.463600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.422000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.445300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.482100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.489100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.439200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.490200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.499200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.456700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.521600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.533300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.376200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.494400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.493300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.529500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.538500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.558100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.578400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.477500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.597400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.518000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.496500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.559400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.539300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.458500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.411400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.425400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.398400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.598400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.494900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.445300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.548700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.533100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.437900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.497900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.513400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.425600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.546400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.481300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.515400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.380300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.442700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.411000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.444800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.394800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.415000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.492900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.343100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.412100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.482400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.511900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.476100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.428700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.418600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.416800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.421200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.502900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.462000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.475500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.499200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.479400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.458100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.402000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.509800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.403400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.428200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.468600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.462900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.427100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.572400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.537300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.409800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.411600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.499100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.451500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.406700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.380800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.568100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.473100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.499400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.435900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.549200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.538500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.521800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.389100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.497100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.402100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.496900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.505700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.554800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.507200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.416700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.449000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.463900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.378500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.502800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.443500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.510700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.443300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.421000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.459300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.494500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.536300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.456500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.440500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.457200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.382500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.408700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.488300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.484700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.428000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.528100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.418100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.465800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.505600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.429100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.472900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.472400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.401000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.513500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.455400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.499900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.461700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2: weight_decay set to 0.003323\n",
            "Iteration 3: weight_decay set to 0.003482\n",
            "Iteration 4: weight_decay set to 0.003640\n",
            "Iteration 5: weight_decay set to 0.003797\n",
            "Iteration 6: weight_decay set to 0.003953\n",
            "Iteration 7: weight_decay set to 0.004107\n",
            "Iteration 8: weight_decay set to 0.004260\n",
            "Iteration 9: weight_decay set to 0.004412\n",
            "Iteration 10: weight_decay set to 0.004562\n",
            "Iteration 11: weight_decay set to 0.004712\n",
            "Iteration 12: weight_decay set to 0.004860\n",
            "Iteration 13: weight_decay set to 0.005007\n",
            "Iteration 14: weight_decay set to 0.005153\n",
            "Iteration 15: weight_decay set to 0.005297\n",
            "Iteration 16: weight_decay set to 0.005440\n",
            "Iteration 17: weight_decay set to 0.005582\n",
            "Iteration 18: weight_decay set to 0.005723\n",
            "Iteration 19: weight_decay set to 0.005862\n",
            "Iteration 20: weight_decay set to 0.006000\n",
            "Iteration 21: weight_decay set to 0.006137\n",
            "Iteration 22: weight_decay set to 0.006273\n",
            "Iteration 23: weight_decay set to 0.006407\n",
            "Iteration 24: weight_decay set to 0.006540\n",
            "Iteration 25: weight_decay set to 0.006672\n",
            "Iteration 26: weight_decay set to 0.006802\n",
            "Iteration 27: weight_decay set to 0.006932\n",
            "Iteration 28: weight_decay set to 0.007060\n",
            "Iteration 29: weight_decay set to 0.007187\n",
            "Iteration 30: weight_decay set to 0.007313\n",
            "Iteration 31: weight_decay set to 0.007437\n",
            "Iteration 32: weight_decay set to 0.007560\n",
            "Iteration 33: weight_decay set to 0.007682\n",
            "Iteration 34: weight_decay set to 0.007803\n",
            "Iteration 35: weight_decay set to 0.007922\n",
            "Iteration 36: weight_decay set to 0.008040\n",
            "Iteration 37: weight_decay set to 0.008157\n",
            "Iteration 38: weight_decay set to 0.008273\n",
            "Iteration 39: weight_decay set to 0.008387\n",
            "Iteration 40: weight_decay set to 0.008500\n",
            "Iteration 41: weight_decay set to 0.008612\n",
            "Iteration 42: weight_decay set to 0.008723\n",
            "Iteration 43: weight_decay set to 0.008832\n",
            "Iteration 44: weight_decay set to 0.008940\n",
            "Iteration 45: weight_decay set to 0.009047\n",
            "Iteration 46: weight_decay set to 0.009153\n",
            "Iteration 47: weight_decay set to 0.009257\n",
            "Iteration 48: weight_decay set to 0.009360\n",
            "Iteration 49: weight_decay set to 0.009462\n",
            "Iteration 50: weight_decay set to 0.009562\n",
            "Iteration 51: weight_decay set to 0.009662\n",
            "Iteration 52: weight_decay set to 0.009760\n",
            "Iteration 53: weight_decay set to 0.009857\n",
            "Iteration 54: weight_decay set to 0.009952\n",
            "Iteration 55: weight_decay set to 0.010047\n",
            "Iteration 56: weight_decay set to 0.010140\n",
            "Iteration 57: weight_decay set to 0.010232\n",
            "Iteration 58: weight_decay set to 0.010323\n",
            "Iteration 59: weight_decay set to 0.010412\n",
            "Iteration 60: weight_decay set to 0.010500\n",
            "Iteration 61: weight_decay set to 0.010587\n",
            "Iteration 62: weight_decay set to 0.010672\n",
            "Iteration 63: weight_decay set to 0.010757\n",
            "Iteration 64: weight_decay set to 0.010840\n",
            "Iteration 65: weight_decay set to 0.010922\n",
            "Iteration 66: weight_decay set to 0.011003\n",
            "Iteration 67: weight_decay set to 0.011082\n",
            "Iteration 68: weight_decay set to 0.011160\n",
            "Iteration 69: weight_decay set to 0.011237\n",
            "Iteration 70: weight_decay set to 0.011312\n",
            "Iteration 71: weight_decay set to 0.011387\n",
            "Iteration 72: weight_decay set to 0.011460\n",
            "Iteration 73: weight_decay set to 0.011532\n",
            "Iteration 74: weight_decay set to 0.011602\n",
            "Iteration 75: weight_decay set to 0.011672\n",
            "Iteration 76: weight_decay set to 0.011740\n",
            "Iteration 77: weight_decay set to 0.011807\n",
            "Iteration 78: weight_decay set to 0.011872\n",
            "Iteration 79: weight_decay set to 0.011937\n",
            "Iteration 80: weight_decay set to 0.012000\n",
            "Iteration 81: weight_decay set to 0.012062\n",
            "Iteration 82: weight_decay set to 0.012122\n",
            "Iteration 83: weight_decay set to 0.012182\n",
            "Iteration 84: weight_decay set to 0.012240\n",
            "Iteration 85: weight_decay set to 0.012297\n",
            "Iteration 86: weight_decay set to 0.012353\n",
            "Iteration 87: weight_decay set to 0.012407\n",
            "Iteration 88: weight_decay set to 0.012460\n",
            "Iteration 89: weight_decay set to 0.012512\n",
            "Iteration 90: weight_decay set to 0.012563\n",
            "Iteration 91: weight_decay set to 0.012612\n",
            "Iteration 92: weight_decay set to 0.012660\n",
            "Iteration 93: weight_decay set to 0.012707\n",
            "Iteration 94: weight_decay set to 0.012752\n",
            "Iteration 95: weight_decay set to 0.012797\n",
            "Iteration 96: weight_decay set to 0.012840\n",
            "Iteration 97: weight_decay set to 0.012882\n",
            "Iteration 98: weight_decay set to 0.012922\n",
            "Iteration 99: weight_decay set to 0.012962\n",
            "Iteration 100: weight_decay set to 0.013000\n",
            "Iteration 101: weight_decay set to 0.013037\n",
            "Iteration 102: weight_decay set to 0.013073\n",
            "Iteration 103: weight_decay set to 0.013107\n",
            "Iteration 104: weight_decay set to 0.013140\n",
            "Iteration 105: weight_decay set to 0.013172\n",
            "Iteration 106: weight_decay set to 0.013203\n",
            "Iteration 107: weight_decay set to 0.013232\n",
            "Iteration 108: weight_decay set to 0.013260\n",
            "Iteration 109: weight_decay set to 0.013287\n",
            "Iteration 110: weight_decay set to 0.013312\n",
            "Iteration 111: weight_decay set to 0.013337\n",
            "Iteration 112: weight_decay set to 0.013360\n",
            "Iteration 113: weight_decay set to 0.013382\n",
            "Iteration 114: weight_decay set to 0.013403\n",
            "Iteration 115: weight_decay set to 0.013422\n",
            "Iteration 116: weight_decay set to 0.013440\n",
            "Iteration 117: weight_decay set to 0.013457\n",
            "Iteration 118: weight_decay set to 0.013473\n",
            "Iteration 119: weight_decay set to 0.013487\n",
            "Iteration 120: weight_decay set to 0.013500\n",
            "Iteration 121: weight_decay set to 0.013512\n",
            "Iteration 122: weight_decay set to 0.013522\n",
            "Iteration 123: weight_decay set to 0.013532\n",
            "Iteration 124: weight_decay set to 0.013540\n",
            "Iteration 125: weight_decay set to 0.013547\n",
            "Iteration 126: weight_decay set to 0.013553\n",
            "Iteration 127: weight_decay set to 0.013557\n",
            "Iteration 128: weight_decay set to 0.013560\n",
            "Iteration 129: weight_decay set to 0.013562\n",
            "Iteration 130: weight_decay set to 0.013562\n",
            "Iteration 131: weight_decay set to 0.013562\n",
            "Iteration 132: weight_decay set to 0.013560\n",
            "Iteration 133: weight_decay set to 0.013557\n",
            "Iteration 134: weight_decay set to 0.013553\n",
            "Iteration 135: weight_decay set to 0.013547\n",
            "Iteration 136: weight_decay set to 0.013540\n",
            "Iteration 137: weight_decay set to 0.013532\n",
            "Iteration 138: weight_decay set to 0.013522\n",
            "Iteration 139: weight_decay set to 0.013512\n",
            "Iteration 140: weight_decay set to 0.013500\n",
            "Iteration 141: weight_decay set to 0.013487\n",
            "Iteration 142: weight_decay set to 0.013473\n",
            "Iteration 143: weight_decay set to 0.013457\n",
            "Iteration 144: weight_decay set to 0.013440\n",
            "Iteration 145: weight_decay set to 0.013422\n",
            "Iteration 146: weight_decay set to 0.013403\n",
            "Iteration 147: weight_decay set to 0.013382\n",
            "Iteration 148: weight_decay set to 0.013360\n",
            "Iteration 149: weight_decay set to 0.013337\n",
            "Iteration 150: weight_decay set to 0.013312\n",
            "Iteration 151: weight_decay set to 0.013287\n",
            "Iteration 152: weight_decay set to 0.013260\n",
            "Iteration 153: weight_decay set to 0.013232\n",
            "Iteration 154: weight_decay set to 0.013203\n",
            "Iteration 155: weight_decay set to 0.013172\n",
            "Iteration 156: weight_decay set to 0.013140\n",
            "Iteration 157: weight_decay set to 0.013107\n",
            "Iteration 158: weight_decay set to 0.013073\n",
            "Iteration 159: weight_decay set to 0.013037\n",
            "Iteration 160: weight_decay set to 0.013000\n",
            "Iteration 161: weight_decay set to 0.012962\n",
            "Iteration 162: weight_decay set to 0.012922\n",
            "Iteration 163: weight_decay set to 0.012882\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ea62c8d74f50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mChebyshevWeightDecayCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheb_coeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/llamalora_model_chebyshev_cot\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/llamalora_modelchebyshev_cot\")"
      ],
      "metadata": {
        "id": "93K4GKNuUM7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4e122b-8941-44e3-d0f4-00e449f8fdc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/llamalora_modelchebyshev_cot/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/llamalora_modelchebyshev_cot/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/llamalora_modelchebyshev_cot/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mpm9Wg7nUM-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "datatest=pd.read_csv(\"/content/drive/MyDrive/CodeMMLU_sample/b6_test_data.csv\")\n",
        "list_task_ids=[]\n",
        "list_answers=[]\n",
        "\n",
        "for index, row in datatest.iterrows():\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            task_instruction, # instruction\n",
        "            str(row[\"question\"]), # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "    list_task_ids.append(str(row[\"task_id\"]))\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "    list_answers.append(tokenizer.batch_decode(outputs)[0][-16])\n",
        "\n",
        "\n",
        "# Unsloth: Input IDs of length 2260 > the model's max sequence length of 2048.\n",
        "# We shall truncate it ourselves. It's imperative if you correct this issue first"
      ],
      "metadata": {
        "id": "VWP9yRuvKfFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_answers_upcase=[]\n",
        "for i in list_answers:\n",
        "    if i not in \"ABCD\":\n",
        "      print (i)\n",
        "    list_answers_upcase.append(i.upper())\n",
        "# list_answers_upcase"
      ],
      "metadata": {
        "id": "ghmFXga-KfFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3fc7116-67c7-47ea-b109-c04a7062bf82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('submissioninfer_cot_066epoch_sup_unsup_160.csv', 'w', encoding=\"utf8\", newline='') as myfile:\n",
        "    wr = csv.writer(myfile)\n",
        "    wr.writerow((\"task_id\", \"answer\"))\n",
        "    wr.writerows(zip(list_task_ids,list_answers_upcase))"
      ],
      "metadata": {
        "id": "cy0arLyoKfFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(outputs)[0]####[-16]"
      ],
      "metadata": {
        "id": "xjcUiyViUNAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "a6b4cd63-fbd4-465a-f9bf-a1dd214236ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nYou are an AI model that can answer coding-related multiple-choice programming-related questions as accurately as possible base on context (Includes the question after the 'Question:' section and the choices enclosed in square brackets are indexed alphabetically A B C D E F G H... There is only 1 correct answer and only 1 uppercase letter is allowed strictly) If you don't know  the answers choose A\\n\\n### Input:\\nQuestion: Consider a  binary min heap containing n elements and every node is having  degree 2 ( i.e. full binary min heap tree). What is the probability of finding the largest element at the last level?\\n['1/2', '1', '1/n', '1/2^n']\\n\\nAnswer: \\n\\n### Response:\\nD<|end_of_text|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJ_vSB4uUNB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChebyShev\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xgTwKqVi0xNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    import torch\n",
        "    max_seq_length = 2548 # Choose any! We auto support RoPE Scaling internally!\n",
        "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/drive/MyDrive/lora_model2_chebyshev\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "        device_map = {\"\": 0}, ####\"auto\",\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# # alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "# inputs = tokenizer(\n",
        "# [\n",
        "#     alpaca_prompt.format(\n",
        "#         task_instruction, # instruction\n",
        "#         \"Question: Consider the following array of elements. 〈89, 19, 50, 17, 12, 15, 2, 5, 7, 11, 6, 9, 100〉. The minimum number of interchanges needed to convert it into a max-heap is ['4', '5', '2', '3'] Answer:\", # input\n",
        "#         \"hello\", # output - leave this blank for generation!\n",
        "#     )\n",
        "# ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# text_streamer = TextStreamer(tokenizer)\n",
        "# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "eSiPvHTZ00w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length=2548\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "task_instruction=\"You are an AI model that can answer coding-related multiple-choice programming-related questions as accurately as possible base on context (Includes the question after the 'Question:' section and the choices enclosed in square brackets are indexed alphabetically A B C D E F G H... There is only 1 correct answer and only 1 uppercase letter is allowed strictly) Last character of response must be your answer\"\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "candidatemapping={\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\":6, \"H\":7}\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/CodeMMLU_sample/b6_train_data.csv\")\n",
        "\n",
        "list_data=[]\n",
        "for index, row in data.iterrows():\n",
        "    if  str(row['answer']).split(\" \")[-1] in \"ABCDEFGH\":\n",
        "      list_data.append({'input': str(row['question']),'output': str(row['answer']).split(\" \")[-1]})\n",
        "dataset_fpt=Dataset.from_list(list_data)\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    # The error ArrowInvalid: Column 2 named text expected length 1000 but got length 122 arises because the formatting_prompts_func is likely designed to process datasets with a specific structure that the dataset variable does not conform to. This function may be expecting the dataset to contain 1000 rows, but it encounters only 122 in the input.\n",
        "    # Instructions: The original code was passing task_instruction (a single string) to the zip function, which expects iterables of the same length. I changed it to create a list of task_instruction repeated for the length of the examples['input'] list.\n",
        "    # Iterating: The formatting_prompts_func is called by the map method and receives a batch of examples. The function should be adapted to handle batched data instead of individual examples. This is done by replacing the original code with a loop that iterates over the batch of examples and applies the prompt template.\n",
        "    # By making these changes, we ensure that the formatting_prompts_func is applied correctly to each example in the dataset, avoiding the error.\n",
        "    instructions = [task_instruction] * len(examples['input'])\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    out_extra=[]#########################\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        # if len(text) < max_length:\n",
        "        #     text = text.ljust(max_length, ' ')\n",
        "        #     print(len(text))\n",
        "        texts.append(text)\n",
        "        out_extra.append(candidatemapping[output])\n",
        "    return { \"text\" : texts,\"task\":[1]* len(examples['input']) }\n",
        "\n",
        "dataset = dataset_fpt.map(formatting_prompts_func, batched = True,)\n",
        "\n"
      ],
      "metadata": {
        "id": "uu-SSui707Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import TrainerCallback\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "class ChebyshevWeightDecayCallback(TrainerCallback):\n",
        "    def __init__(self, total_steps, coeffs):\n",
        "        \"\"\"\n",
        "        total_steps: tổng số bước (iterations) huấn luyện.\n",
        "        coeffs: danh sách các hệ số Chebyshev [c0, c1, c2, ...] cho đa thức Chebyshev T0, T1, T2,...\n",
        "        \"\"\"\n",
        "        self.total_steps = total_steps\n",
        "        self.coeffs = coeffs\n",
        "\n",
        "    def compute_weight_decay(self, current_step):\n",
        "        # Map current_step từ [0, total_steps] sang [-1, 1]\n",
        "        x = 2 * (current_step / self.total_steps) - 1\n",
        "        # Tính giá trị của chuỗi Chebyshev tại x:\n",
        "        # wd = c0*T0(x) + c1*T1(x) + c2*T2(x) + ...\n",
        "        wd = np.polynomial.chebyshev.chebval(x, self.coeffs)\n",
        "        return float(wd)\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        current_step = state.global_step\n",
        "        new_wd = self.compute_weight_decay(current_step)\n",
        "        optimizer = kwargs.get(\"optimizer\", None)\n",
        "        if optimizer is not None:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group[\"weight_decay\"] = new_wd\n",
        "        # In ra để debug\n",
        "        print(f\"Iteration {current_step}: weight_decay set to {new_wd:.6f}\")\n",
        "        return control\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer  # hoặc import từ nguồn tương ứng\n",
        "\n",
        "# Ví dụ: tổng số bước huấn luyện\n",
        "total_steps = 160\n",
        "# Ví dụ hệ số Chebyshev: các giá trị này bạn có thể tùy chỉnh theo nhu cầu\n",
        "cheb_coeffs = [0.01, 0.005, -0.002]  # Tương ứng với c0, c1, c2\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,  # Giúp training nhanh hơn nếu chuỗi ngắn.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = total_steps,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,  # Giá trị ban đầu, sẽ được cập nhật qua callback.\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",  # Nếu không dùng WandB, v.v.\n",
        "    ),\n",
        "    callbacks = [ChebyshevWeightDecayCallback(total_steps, cheb_coeffs)]\n",
        ")\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d40dbbb49309486aa13cb0585fc76c43",
            "213b67dd18ee4fac9a612c85648e40de",
            "fae91604b50a40d3be39b54a1dcc0466",
            "1c7d60e7fae0442493fb971f3055a052",
            "fd2d7d5ab9b4467e8dfde06825918934",
            "e93be1f4ea4345c58f145d217eb3f492",
            "7aa464c3a84f4b04ad946842b78751a6",
            "a17299418c304784b5e2af09e26be578",
            "a2528b60bac146dab20d16a9a87578d4",
            "2c4c170929324140b041a43ca350e429",
            "9438a38fa59b4a4897021d1ecea6aec6"
          ]
        },
        "id": "9RwItuGkvB4V",
        "outputId": "300f65c4-09bf-4cbe-bdb4-839f5eb1dbb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/3963 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d40dbbb49309486aa13cb0585fc76c43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,963 | Num Epochs = 1 | Total steps = 160\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 80,740,352/7,000,000,000 (1.15% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: weight_decay set to 0.003162\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 24:06, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.283000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.280500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.264300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.226800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.221300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.258900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.211300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.295100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.138700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.410200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.162100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.163000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.283200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.228000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.204600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.173900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.388900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.183300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.312900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.338600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.286500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.197300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.331900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.254300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.253800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.255800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.257000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.403600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.227900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.309900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.240600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.342300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.239300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.380000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.167400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.172700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.225100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.202700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.278300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.281600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.485200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.259500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.202700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.284800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.180900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.130100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.257800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.232000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.235500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.187700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.237800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.255200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.266000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.354900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.241200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.295900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.284600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.254100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.261200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.158900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.358300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.309300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.175400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.223700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.249600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.318600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.328200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.216300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.223200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.472800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.299400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.220400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.247500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.274900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.340000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.274500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.296700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.313200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.311500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.231900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.314700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.241400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.312600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.291000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.304000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.197900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.383800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.341300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.270900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.188600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.230200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.266700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.194400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.357600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.204100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.176500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.282100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.226200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.238400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.310200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.365500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.191200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.323000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.157400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.205000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.305300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.272600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.163500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.214300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.208700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.309200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.336600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.325100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.256600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.467700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.265000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.314000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.216300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.271300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.217600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.305200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.345000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.246700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.430700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.183000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.248900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.288900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.416800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.329200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.162700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.172300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.252200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.207000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.232400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.238900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.226500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.427300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.230100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.295800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.430400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.295000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.230200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.277500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.402600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.288100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.229700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.315200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2: weight_decay set to 0.003323\n",
            "Iteration 3: weight_decay set to 0.003482\n",
            "Iteration 4: weight_decay set to 0.003640\n",
            "Iteration 5: weight_decay set to 0.003797\n",
            "Iteration 6: weight_decay set to 0.003953\n",
            "Iteration 7: weight_decay set to 0.004107\n",
            "Iteration 8: weight_decay set to 0.004260\n",
            "Iteration 9: weight_decay set to 0.004412\n",
            "Iteration 10: weight_decay set to 0.004562\n",
            "Iteration 11: weight_decay set to 0.004712\n",
            "Iteration 12: weight_decay set to 0.004860\n",
            "Iteration 13: weight_decay set to 0.005007\n",
            "Iteration 14: weight_decay set to 0.005153\n",
            "Iteration 15: weight_decay set to 0.005297\n",
            "Iteration 16: weight_decay set to 0.005440\n",
            "Iteration 17: weight_decay set to 0.005582\n",
            "Iteration 18: weight_decay set to 0.005723\n",
            "Iteration 19: weight_decay set to 0.005862\n",
            "Iteration 20: weight_decay set to 0.006000\n",
            "Iteration 21: weight_decay set to 0.006137\n",
            "Iteration 22: weight_decay set to 0.006273\n",
            "Iteration 23: weight_decay set to 0.006407\n",
            "Iteration 24: weight_decay set to 0.006540\n",
            "Iteration 25: weight_decay set to 0.006672\n",
            "Iteration 26: weight_decay set to 0.006802\n",
            "Iteration 27: weight_decay set to 0.006932\n",
            "Iteration 28: weight_decay set to 0.007060\n",
            "Iteration 29: weight_decay set to 0.007187\n",
            "Iteration 30: weight_decay set to 0.007313\n",
            "Iteration 31: weight_decay set to 0.007437\n",
            "Iteration 32: weight_decay set to 0.007560\n",
            "Iteration 33: weight_decay set to 0.007682\n",
            "Iteration 34: weight_decay set to 0.007803\n",
            "Iteration 35: weight_decay set to 0.007922\n",
            "Iteration 36: weight_decay set to 0.008040\n",
            "Iteration 37: weight_decay set to 0.008157\n",
            "Iteration 38: weight_decay set to 0.008273\n",
            "Iteration 39: weight_decay set to 0.008387\n",
            "Iteration 40: weight_decay set to 0.008500\n",
            "Iteration 41: weight_decay set to 0.008612\n",
            "Iteration 42: weight_decay set to 0.008723\n",
            "Iteration 43: weight_decay set to 0.008832\n",
            "Iteration 44: weight_decay set to 0.008940\n",
            "Iteration 45: weight_decay set to 0.009047\n",
            "Iteration 46: weight_decay set to 0.009153\n",
            "Iteration 47: weight_decay set to 0.009257\n",
            "Iteration 48: weight_decay set to 0.009360\n",
            "Iteration 49: weight_decay set to 0.009462\n",
            "Iteration 50: weight_decay set to 0.009562\n",
            "Iteration 51: weight_decay set to 0.009662\n",
            "Iteration 52: weight_decay set to 0.009760\n",
            "Iteration 53: weight_decay set to 0.009857\n",
            "Iteration 54: weight_decay set to 0.009952\n",
            "Iteration 55: weight_decay set to 0.010047\n",
            "Iteration 56: weight_decay set to 0.010140\n",
            "Iteration 57: weight_decay set to 0.010232\n",
            "Iteration 58: weight_decay set to 0.010323\n",
            "Iteration 59: weight_decay set to 0.010412\n",
            "Iteration 60: weight_decay set to 0.010500\n",
            "Iteration 61: weight_decay set to 0.010587\n",
            "Iteration 62: weight_decay set to 0.010672\n",
            "Iteration 63: weight_decay set to 0.010757\n",
            "Iteration 64: weight_decay set to 0.010840\n",
            "Iteration 65: weight_decay set to 0.010922\n",
            "Iteration 66: weight_decay set to 0.011003\n",
            "Iteration 67: weight_decay set to 0.011082\n",
            "Iteration 68: weight_decay set to 0.011160\n",
            "Iteration 69: weight_decay set to 0.011237\n",
            "Iteration 70: weight_decay set to 0.011312\n",
            "Iteration 71: weight_decay set to 0.011387\n",
            "Iteration 72: weight_decay set to 0.011460\n",
            "Iteration 73: weight_decay set to 0.011532\n",
            "Iteration 74: weight_decay set to 0.011602\n",
            "Iteration 75: weight_decay set to 0.011672\n",
            "Iteration 76: weight_decay set to 0.011740\n",
            "Iteration 77: weight_decay set to 0.011807\n",
            "Iteration 78: weight_decay set to 0.011872\n",
            "Iteration 79: weight_decay set to 0.011937\n",
            "Iteration 80: weight_decay set to 0.012000\n",
            "Iteration 81: weight_decay set to 0.012062\n",
            "Iteration 82: weight_decay set to 0.012122\n",
            "Iteration 83: weight_decay set to 0.012182\n",
            "Iteration 84: weight_decay set to 0.012240\n",
            "Iteration 85: weight_decay set to 0.012297\n",
            "Iteration 86: weight_decay set to 0.012353\n",
            "Iteration 87: weight_decay set to 0.012407\n",
            "Iteration 88: weight_decay set to 0.012460\n",
            "Iteration 89: weight_decay set to 0.012512\n",
            "Iteration 90: weight_decay set to 0.012563\n",
            "Iteration 91: weight_decay set to 0.012612\n",
            "Iteration 92: weight_decay set to 0.012660\n",
            "Iteration 93: weight_decay set to 0.012707\n",
            "Iteration 94: weight_decay set to 0.012752\n",
            "Iteration 95: weight_decay set to 0.012797\n",
            "Iteration 96: weight_decay set to 0.012840\n",
            "Iteration 97: weight_decay set to 0.012882\n",
            "Iteration 98: weight_decay set to 0.012922\n",
            "Iteration 99: weight_decay set to 0.012962\n",
            "Iteration 100: weight_decay set to 0.013000\n",
            "Iteration 101: weight_decay set to 0.013037\n",
            "Iteration 102: weight_decay set to 0.013073\n",
            "Iteration 103: weight_decay set to 0.013107\n",
            "Iteration 104: weight_decay set to 0.013140\n",
            "Iteration 105: weight_decay set to 0.013172\n",
            "Iteration 106: weight_decay set to 0.013203\n",
            "Iteration 107: weight_decay set to 0.013232\n",
            "Iteration 108: weight_decay set to 0.013260\n",
            "Iteration 109: weight_decay set to 0.013287\n",
            "Iteration 110: weight_decay set to 0.013312\n",
            "Iteration 111: weight_decay set to 0.013337\n",
            "Iteration 112: weight_decay set to 0.013360\n",
            "Iteration 113: weight_decay set to 0.013382\n",
            "Iteration 114: weight_decay set to 0.013403\n",
            "Iteration 115: weight_decay set to 0.013422\n",
            "Iteration 116: weight_decay set to 0.013440\n",
            "Iteration 117: weight_decay set to 0.013457\n",
            "Iteration 118: weight_decay set to 0.013473\n",
            "Iteration 119: weight_decay set to 0.013487\n",
            "Iteration 120: weight_decay set to 0.013500\n",
            "Iteration 121: weight_decay set to 0.013512\n",
            "Iteration 122: weight_decay set to 0.013522\n",
            "Iteration 123: weight_decay set to 0.013532\n",
            "Iteration 124: weight_decay set to 0.013540\n",
            "Iteration 125: weight_decay set to 0.013547\n",
            "Iteration 126: weight_decay set to 0.013553\n",
            "Iteration 127: weight_decay set to 0.013557\n",
            "Iteration 128: weight_decay set to 0.013560\n",
            "Iteration 129: weight_decay set to 0.013562\n",
            "Iteration 130: weight_decay set to 0.013562\n",
            "Iteration 131: weight_decay set to 0.013562\n",
            "Iteration 132: weight_decay set to 0.013560\n",
            "Iteration 133: weight_decay set to 0.013557\n",
            "Iteration 134: weight_decay set to 0.013553\n",
            "Iteration 135: weight_decay set to 0.013547\n",
            "Iteration 136: weight_decay set to 0.013540\n",
            "Iteration 137: weight_decay set to 0.013532\n",
            "Iteration 138: weight_decay set to 0.013522\n",
            "Iteration 139: weight_decay set to 0.013512\n",
            "Iteration 140: weight_decay set to 0.013500\n",
            "Iteration 141: weight_decay set to 0.013487\n",
            "Iteration 142: weight_decay set to 0.013473\n",
            "Iteration 143: weight_decay set to 0.013457\n",
            "Iteration 144: weight_decay set to 0.013440\n",
            "Iteration 145: weight_decay set to 0.013422\n",
            "Iteration 146: weight_decay set to 0.013403\n",
            "Iteration 147: weight_decay set to 0.013382\n",
            "Iteration 148: weight_decay set to 0.013360\n",
            "Iteration 149: weight_decay set to 0.013337\n",
            "Iteration 150: weight_decay set to 0.013312\n",
            "Iteration 151: weight_decay set to 0.013287\n",
            "Iteration 152: weight_decay set to 0.013260\n",
            "Iteration 153: weight_decay set to 0.013232\n",
            "Iteration 154: weight_decay set to 0.013203\n",
            "Iteration 155: weight_decay set to 0.013172\n",
            "Iteration 156: weight_decay set to 0.013140\n",
            "Iteration 157: weight_decay set to 0.013107\n",
            "Iteration 158: weight_decay set to 0.013073\n",
            "Iteration 159: weight_decay set to 0.013037\n",
            "Iteration 160: weight_decay set to 0.013000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/lora_model2_chebyshev_cot_learn\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/lora_model2chebyshev_cot_learn\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "xa3PzKvv07Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KD"
      ],
      "metadata": {
        "id": "fniVs3EWAymd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    import torch\n",
        "    max_seq_length = 2548 # Choose any! We auto support RoPE Scaling internally!\n",
        "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/drive/MyDrive/lora_model2_chebyshev\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "        device_map = {\"\": 0}, ####\"auto\",\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "teacher_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/lora_model2_chebyshev\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    device_map = {\"\": 0}, ####\"auto\",\n",
        ")\n",
        "FastLanguageModel.for_inference(teacher_model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "09ff37f9da1f4ef486ce8c6a527f2083",
            "95b69a9599d844b7b4ff3f3931b340f9",
            "1ced5b6390c94dee8a39a482d67f4b5b",
            "afa2dfbdc3674430b47870e803f73fc0",
            "3f2b23ac99f443b08c9c9760f17a8f36",
            "358a2e6108324cad8de8cff1eb15b898",
            "f9ddab8c5b534dd18b462e46317e9047",
            "258b5dcd61c94c7695b4415f98a9d2de",
            "df2f9487d8d8429e954b3efac20c53ee",
            "7dcd3ed01052402496382057d6678d02",
            "340e8f65deb5411ca56164aaca465e42",
            "631f074d22ac48988fa5522bd0342b29",
            "3d19a2b70907486ab4706319b80ee91f",
            "6b4b1d3ba0594190bb3cdefe98139809",
            "236ba080c44349f8a0fc695be7ea92da",
            "f1b9a7f50015455cb226a5d1d1194e14",
            "acef5f5fe26740128013f1254c49df14",
            "e4743bdc3d7747c4a40c680aa54e1853",
            "f9302cf6965744f7ba319bc9f7b0e64b",
            "a51443e8c3234919b6f5824e5f564361",
            "1b1b565836e44ea2b2d704d9422875f5",
            "83d800a9887e45c9b21eb40563fd3d99"
          ]
        },
        "id": "WxvSHd9bBvXX",
        "outputId": "d854b4da-1ce8-4490-b000-ec18c66a8ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09ff37f9da1f4ef486ce8c6a527f2083"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "631f074d22ac48988fa5522bd0342b29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (2-3): 2 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (4): Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (5): Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (6): Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (7-24): 18 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (25-26): 2 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (27): Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length=2548\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "task_instruction=\"You are an AI model that can answer coding-related multiple-choice programming-related questions as accurately as possible base on context (Includes the question after the 'Question:' section and the choices enclosed in square brackets are indexed alphabetically A B C D E F G H... There is only 1 correct answer and only 1 uppercase letter is allowed strictly) Last character of response must be your answer\"\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "candidatemapping={\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\":6, \"H\":7}\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/CodeMMLU_sample/b6_train_data.csv\")\n",
        "\n",
        "list_data=[]\n",
        "for index, row in data.iterrows():\n",
        "    if  str(row['answer']).split(\" \")[-1] in \"ABCDEFGH\":\n",
        "      list_data.append({'input': str(row['question']),'output': str(row['answer']).split(\" \")[-1]})\n",
        "dataset_fpt=Dataset.from_list(list_data)\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    # The error ArrowInvalid: Column 2 named text expected length 1000 but got length 122 arises because the formatting_prompts_func is likely designed to process datasets with a specific structure that the dataset variable does not conform to. This function may be expecting the dataset to contain 1000 rows, but it encounters only 122 in the input.\n",
        "    # Instructions: The original code was passing task_instruction (a single string) to the zip function, which expects iterables of the same length. I changed it to create a list of task_instruction repeated for the length of the examples['input'] list.\n",
        "    # Iterating: The formatting_prompts_func is called by the map method and receives a batch of examples. The function should be adapted to handle batched data instead of individual examples. This is done by replacing the original code with a loop that iterates over the batch of examples and applies the prompt template.\n",
        "    # By making these changes, we ensure that the formatting_prompts_func is applied correctly to each example in the dataset, avoiding the error.\n",
        "    instructions = [task_instruction] * len(examples['input'])\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    out_extra=[]#########################\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        # if len(text) < max_length:\n",
        "        #     text = text.ljust(max_length, ' ')\n",
        "        #     print(len(text))\n",
        "        texts.append(text)\n",
        "        out_extra.append(candidatemapping[output])\n",
        "    return { \"text\" : texts,\"task\":[1]* len(examples['input']) }\n",
        "\n",
        "dataset_fpt = dataset_fpt.map(formatting_prompts_func, batched = True,)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8d489ecb11c0448aa308dfcfe59a08c3",
            "51c50cc3561c46158af199ab0fc529e2",
            "86f6d7fa75b6403ebefb35c4a2fa3f73",
            "e56751adbd214c1d8271b1bc1ab531c2",
            "adac3afab50241d9a6cc4f86ad0ebc50",
            "2fb2cde6117e4ddba9920749b585870e",
            "03f861866b1143a59f817ef3c4acc36a",
            "de7acb98dd8c4cbe8e7bc7bb240c2127",
            "268dc340186740d08b97528f89c0b137",
            "e35356e9272d4364b3d6a22b5def5d1f",
            "39aa9c1be9524c2d83a60f08ab179c8d"
          ]
        },
        "id": "FjUII_yiB6pE",
        "outputId": "fda7864b-afeb-4e7f-e74d-b598dc0e908a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3949 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d489ecb11c0448aa308dfcfe59a08c3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_cc=dataset_fpt"
      ],
      "metadata": {
        "id": "ygZNB9OtCqcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fl0-rcTCowv",
        "outputId": "0fbce5ab-ded0-465e-b88c-eaed15a12d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "13<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
      ],
      "metadata": {
        "id": "JW35GsnsDita"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ---------------------\n",
        "# Hàm tính KD loss trên logits với temperature scaling\n",
        "def compute_kd_loss_logits(student_logits, teacher_logits, temperature=2.0):\n",
        "    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
        "    kd_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
        "    return kd_loss\n",
        "\n",
        "# Hàm tính KD loss trên hidden states (sử dụng MSE loss)\n",
        "def compute_kd_loss_hidden(student_hidden, teacher_hidden, mask=None):\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(-1)  # (batch, seq_length, 1)\n",
        "        loss = F.mse_loss(student_hidden * mask, teacher_hidden * mask, reduction='sum')\n",
        "        loss = loss / mask.sum().clamp(min=1.0)\n",
        "    else:\n",
        "        loss = F.mse_loss(student_hidden, teacher_hidden, reduction='mean')\n",
        "    return loss\n",
        "\n",
        "# Hàm tổng hợp KD loss\n",
        "def compute_total_kd_loss(student_logits, teacher_logits, student_hidden, teacher_hidden, temperature=2.0, lambda_hidden=1.0):\n",
        "    loss_logits = compute_kd_loss_logits(student_logits, teacher_logits, temperature)\n",
        "    loss_hidden = compute_kd_loss_hidden(student_hidden, teacher_hidden)\n",
        "    total_loss = loss_logits + lambda_hidden * loss_hidden\n",
        "    return total_loss\n",
        "\n",
        "# ---------------------\n",
        "# Custom KD Trainer dựa trên SFTTrainer\n",
        "class CustomKDTrainer(SFTTrainer):\n",
        "    def __init__(self, teacher_model, kd_alpha=0.5, kd_temperature=2.0, lambda_hidden=0.02, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        teacher_model: Teacher model đã được pre-trained và giữ ở chế độ eval.\n",
        "        kd_alpha: Trọng số cân bằng giữa LM loss và KD loss.\n",
        "                  Tổng loss = (1 - kd_alpha)*LM_loss + kd_alpha*KD_loss.\n",
        "        kd_temperature: Temperature scaling cho KD loss.\n",
        "        lambda_hidden: Hệ số cân bằng cho KD loss trên hidden states.\n",
        "        \"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher_model = teacher_model\n",
        "        self.teacher_model.eval()  # Không update teacher\n",
        "        self.kd_alpha = kd_alpha\n",
        "        self.kd_temperature = kd_temperature\n",
        "        self.lambda_hidden = lambda_hidden\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False,  **kwargs):\n",
        "        # Forward pass của student\n",
        "        student_outputs = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            labels=inputs[\"input_ids\"],  # causal LM loss: dự đoán token tiếp theo\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        lm_loss = student_outputs.loss if student_outputs.loss is not None else 0.0\n",
        "        student_logits = student_outputs.logits            # shape: (batch, seq_length, vocab_size)\n",
        "        student_hidden = student_outputs.hidden_states[-1]   # shape: (batch, seq_length, hidden_dim)\n",
        "\n",
        "        # Forward pass của teacher (không cập nhật trọng số)\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = self.teacher_model(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                output_hidden_states=True,\n",
        "                return_dict=True,\n",
        "            )\n",
        "        # teacher_logits = teacher_outputs.logits              # shape: (batch, seq_length, vocab_size)\n",
        "        # teacher_hidden = teacher_outputs.hidden_states[-1]     # shape: (batch, seq_length, hidden_dim)\n",
        "\n",
        "        # *** Cast teacher outputs to Float32 to avoid dtype mismatch ***  RuntimeError: Found dtype BFloat16 but expected Float\n",
        "        teacher_logits = teacher_outputs.logits.type(torch.float32)\n",
        "        teacher_hidden = teacher_outputs.hidden_states[-1].type(torch.float32)\n",
        "\n",
        "        # Tính KD loss: kết hợp cả logits và hidden states\n",
        "        kd_loss = compute_total_kd_loss(\n",
        "            student_logits, teacher_logits, student_hidden, teacher_hidden,\n",
        "            temperature=self.kd_temperature,\n",
        "            lambda_hidden=self.lambda_hidden\n",
        "        )\n",
        "\n",
        "        total_loss = (1 - self.kd_alpha) * lm_loss + self.kd_alpha * kd_loss\n",
        "        return (total_loss, student_outputs) if return_outputs else total_loss\n",
        "\n",
        "# # ---------------------\n",
        "# # Ví dụ sử dụng\n",
        "# # Giả sử bạn có teacher_model và student_model cùng kiến trúc LLaMA hoặc mô hình causal LM khác.\n",
        "# student_model_name = \"your-student-llama-model\"  # Thay bằng tên model của bạn\n",
        "# teacher_model_name = \"your-teacher-llama-model\"    # Thay bằng tên model của teacher\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
        "# student_model = AutoModelForCausalLM.from_pretrained(student_model_name)\n",
        "# teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name)\n",
        "\n",
        "# # Giả sử bạn đã chuẩn bị train_dataset với các trường: \"input_ids\", \"attention_mask\", \"labels\"\n",
        "# # Ví dụ, train_dataset được tạo từ một dataset của Hugging Face sau khi tokenize.\n",
        "# # training_args\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./kd_finetuned_llama\",\n",
        "#     per_device_train_batch_size=2,\n",
        "#     num_train_epochs=3,\n",
        "#     learning_rate=2e-4,\n",
        "#     logging_steps=10,\n",
        "#     fp16=True,\n",
        "# )\n",
        "\n",
        "# trainer = CustomKDTrainer(\n",
        "#     teacher_model=teacher_model,\n",
        "#     model=student_model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     kd_alpha=0.5,         # Điều chỉnh trọng số giữa LM loss và KD loss\n",
        "#     kd_temperature=2.0,   # Temperature scaling cho KD loss\n",
        "#     lambda_hidden=1.0,    # Hệ số cho KD loss hidden states\n",
        "# )\n",
        "\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "id": "N_tSprqvA1vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import TrainerCallback\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "\n",
        "# Khởi tạo custom trainer với learnable_prompt.\n",
        "trainer = CustomKDTrainer(\n",
        "    # candidate_mapping={\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\":6, \"H\":7},\n",
        "    teacher_model=teacher_model,\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset_cc,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"./prompt_tuning_output\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 80,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.05,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        # output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "        # max_grad_norm = 1.0,\n",
        "    ),\n",
        "    # # data_collator=custom_data_collator,  ############# Sử dụng custom data collator\n",
        "    # callbacks = [ChebyshevWeightDecayCallback(total_steps, cheb_coeffs)]\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2397210ae3994844aede9ef8d5eff5e3",
            "72c87248ee524c27af9d41e6d5ca8f5d",
            "c99ecb44bd01467387c29de4cb0a713e",
            "2015626c03c64aa3aca3811f079356d8",
            "6046ed451a8b48ada37722af83d9ecb8",
            "aafa4f6c31cf4e69a8c94b4919bb6ced",
            "8ea5781f66444b3286b032427d3552c0",
            "6c8ad549180d4c7bbaa7f3dd81da66b3",
            "defae93b8b7240a9b05f59b70d12ee09",
            "a0e77721f9324170a26cb62ab67a986c",
            "6e49d872a4bd4a1fbb851282b5868f79"
          ]
        },
        "id": "JE6K-LjtA2Ph",
        "outputId": "2ce83307-d449-4833-b699-74f4ae4f5bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/3949 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2397210ae3994844aede9ef8d5eff5e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,949 | Num Epochs = 1 | Total steps = 987\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 80,740,352/7,000,000,000 (1.15% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='987' max='987' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [987/987 19:34, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>428.007400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>746.288600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>784.515300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>397.009400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>552.056900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>188.811300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>183.611300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>533.753900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>93.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>478.447800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>119.217100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>456.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>23.843900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>296.996500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>505.300500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>51.479200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>196.222100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>86.358400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>606.035200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>178.668800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>167.452800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>272.764600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>617.135600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>91.300300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>123.172000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>86.487600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>106.262100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>219.542800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>773.592400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>45.873500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>21.977800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>144.986800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>37.054900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>710.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>170.289400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>139.838500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>96.608100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>63.282400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>113.900500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>27.840300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>82.663300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>259.632400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>159.281600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>193.669800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>461.822900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>275.671200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>445.683100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>50.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>314.348800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>340.543600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>216.012100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>234.989600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>339.268900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>349.531200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>23.271100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>214.750700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>112.363400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>313.526400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>56.356200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>121.366100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>135.358500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>53.740400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>89.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>95.455000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>145.261700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>107.490500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>136.121200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>374.734400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>57.240100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>427.288400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>85.569500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>176.428300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>382.414700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>20.646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>144.198700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>247.426500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>77.918100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>20.627800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>230.450500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>250.932400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>515.959100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>86.073400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>22.728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>44.409100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>148.551400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>70.985100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>260.532900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>11.707700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>290.458600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>145.886600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>75.922700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>547.835100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>57.345700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>154.168700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>67.338600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>181.615100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>22.200100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>332.799600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>256.836500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>21.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>273.393500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>54.213400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>274.634600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>15.489700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>61.178700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>53.663200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>106.500700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>67.796400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>215.249100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>160.631800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>153.140500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>120.545400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>142.153300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>265.263700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>598.255100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>190.103900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>135.598500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>161.433800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>84.021700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>43.131700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>157.713100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>256.261600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>104.262100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>186.925800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>227.608000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>108.485900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>205.407900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>233.012000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>134.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>119.074100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>462.996000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>32.040500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>272.621100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>215.360700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>240.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>45.808800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>15.914200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>269.621500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>155.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>641.855900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>37.315400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>135.994500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>375.267400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>87.995800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>87.687700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>127.038900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>31.232700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>116.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>88.698800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>136.336500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>39.377600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>60.498100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>43.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>44.920600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>92.584400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>38.268300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>165.825400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>93.248000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>154.840200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>13.948000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>33.236300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>22.426000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>155.680000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>376.469500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>193.029800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>36.811200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>25.794700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>48.462800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>303.822300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>59.054200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>433.956700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>252.289700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>515.838200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>64.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>174.150400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>87.591300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>110.850600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>59.623800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>231.445300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>95.768300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>24.760200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>47.912700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>140.038400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>514.174900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>18.634100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>335.009200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>230.717500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>135.545600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>80.424600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>122.826600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>43.314800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>323.027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>164.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>62.233100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>150.897600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>102.900700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>63.995700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>11.361500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>29.187300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>92.849000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>210.843900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>192.159100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>138.719600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>165.103900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>87.276300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>120.536800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>135.413300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>136.037600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>192.080300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>89.151100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>44.799000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>48.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>235.471500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>90.687300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>194.487400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>139.855900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>108.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>25.716900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>7.829100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>144.313300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>412.885300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>94.842000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>42.713200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>22.314500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>93.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>113.123000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>247.930600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>96.691400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>149.210400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>92.353400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>254.632400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>116.419700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>75.077300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>119.717200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>133.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>63.891600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>175.494800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>7.045000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>285.909900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>54.738900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>93.067800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>23.681000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>83.789700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>40.642900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>5.044500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>102.849200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>259.515500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>272.870100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>92.411300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>272.667800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>120.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>40.504600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>177.047000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>171.692000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>184.504800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>135.818400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>37.733000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>119.750500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>385.398800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>109.330200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>17.860900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>124.053200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>32.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>12.624900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>77.520400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>655.668600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>21.846900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>363.591100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>43.729000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>24.802100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>166.057800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>128.564400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>107.649300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>125.729600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>115.422200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>371.089800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>6.353200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>22.017100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>75.779700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>69.819200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>50.954600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>128.861500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>104.292900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>195.031700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>20.932000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>26.813500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>131.850800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>18.784500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>136.223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>261.642500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>64.227000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>429.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>135.142100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>74.244700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>213.858000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>309.771900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>175.186300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>146.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>213.649200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>122.805500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>481.340400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>60.124100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>117.326900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>86.248600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>40.764200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>74.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>237.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>40.626700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>202.139500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>142.207000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>345.694800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>179.259500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>87.272100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>101.457100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>110.549700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>82.855200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>142.356200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>72.913600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>124.177400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>934.891200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>263.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>129.174800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>121.842900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>74.600500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>105.760200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>122.091000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>313.551800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>12.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>12.563200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>113.705600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>89.342100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>137.612200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>65.154400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>173.975500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>81.808600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>200.442800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>268.139000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>592.890900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>28.727200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>91.576500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>37.967400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>182.817000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>185.106400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>74.181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>43.986000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>33.655700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>46.942400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>220.580400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>207.146500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>95.569500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>9.010900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>63.920800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>44.148500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>155.200800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>245.810600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>309.565900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>27.068400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>216.077900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>140.575700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>7.523000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>72.108700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>83.779200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>37.056700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>61.647000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>267.399000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>84.181800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>165.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>22.441800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>121.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>129.344900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>140.670300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>141.756900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>57.877900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>77.987300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>137.273700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>153.333000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>17.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>107.954800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>52.713000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>107.112600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>41.387500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>124.328000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>119.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>54.699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>96.392200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>233.467200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>94.172900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>30.071600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>159.974100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>334.796200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>188.298800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>59.934900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>88.036000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>79.614400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>66.188300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>81.865800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>53.913500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>21.719300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>116.494000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>44.531800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>194.257800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>314.212400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>115.851500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>124.648300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>282.880400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>75.563300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>153.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>225.125200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>438.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>35.366800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>209.038500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>80.564400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>96.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>82.752800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>141.995900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>194.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>113.786600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>60.013100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>196.208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>14.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>173.898800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>36.115900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>62.840400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>109.497200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>586.362100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>223.419400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>139.709100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>109.492500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>51.361400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>123.920900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>123.445100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>153.667500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>47.897800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>229.152700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>56.241200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>144.953000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>130.891900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>296.600800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>70.307400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>50.523900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>145.499600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>43.333900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>147.988000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>43.944100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>23.932100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>68.954900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>49.930700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>88.885700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>162.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>64.482100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>80.113400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>326.322200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>198.274600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>125.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>576.908100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>12.294300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>34.105900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>136.599600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>19.237900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>27.908500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>133.517200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>88.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>192.333600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>149.223100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>72.853200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>125.448700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>133.449700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>188.788500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>27.501200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>80.389100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>53.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>118.932100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>100.138600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>37.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>23.380600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>170.808800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>67.139300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>197.111400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>56.903300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>148.953700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>37.758800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>55.226600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>15.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>280.470900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>325.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>20.762800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>47.982700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>167.761700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>231.597700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>39.533500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>99.280900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>97.730300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>182.432100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>103.090300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>4.964100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>76.908700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>41.874100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>157.810600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>160.633800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>367.955000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>156.600100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>88.668500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>21.091900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>20.928800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>62.238700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>148.345100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>140.432400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>51.508400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>36.898900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>47.428000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>97.206800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>54.281500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>115.096900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>127.935100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>22.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>67.691400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>83.809900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>303.109100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>96.843000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>93.810400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>101.578200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>75.236900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>22.058800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>44.984200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>98.299200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>405.958600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>191.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>139.512900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>156.317600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>54.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>215.240700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>96.212100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>85.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>123.913100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>74.945600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>480.419700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>113.584100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>110.666400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>241.787400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>165.515100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>121.276600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>65.130400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>17.113300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>75.056000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>38.033200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>55.793200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>109.585600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>45.319000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>95.618900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>170.813800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>88.613500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>31.978900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>121.195900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>212.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>44.783900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>38.177700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>172.026000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>32.425700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>118.824700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>51.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>77.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>106.283900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>175.277600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>233.826500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>16.757400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>243.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>99.079500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>226.293100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>47.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>93.441800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>17.158200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>8.698900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>18.371000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>94.540600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>40.827900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>152.447600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>17.471600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>52.512300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>160.370300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>153.272000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>88.663000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>430.582500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>221.356900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>16.992100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>58.455900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>113.563800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>275.636400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>41.264900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>211.117400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>43.489000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>30.385500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>86.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>137.087300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>83.973500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>89.871400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>114.353900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>179.615700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>112.126400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>495.123200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>31.959200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>205.942200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>2.377500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>69.379100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>81.316800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>121.191600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>26.123300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>108.281100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>50.119200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>195.716600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>90.477000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>33.713600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>73.123500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>449.328700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>94.751700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>264.629800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>68.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>113.780300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>488.913500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>13.724300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>43.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>152.942500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>306.606900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>144.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>12.033000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>79.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>26.294100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>46.450900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>149.242200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>208.901800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>241.444500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>332.963500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>136.752100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>48.588800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>116.828000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>53.477300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>21.908000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>37.613100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>34.464700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>36.256300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>16.181200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>497.458800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>43.012900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>43.491500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>206.973700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>72.114000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>21.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>55.670600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>262.632500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>93.007300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>5.827500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>132.610400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>623.260400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>122.166100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>243.039300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>117.569900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>656</td>\n",
              "      <td>173.635900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657</td>\n",
              "      <td>145.891400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>658</td>\n",
              "      <td>131.367900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>659</td>\n",
              "      <td>57.605500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>11.927700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>661</td>\n",
              "      <td>61.818500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>662</td>\n",
              "      <td>12.308500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>663</td>\n",
              "      <td>129.143300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>664</td>\n",
              "      <td>152.463200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>665</td>\n",
              "      <td>123.863900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>666</td>\n",
              "      <td>51.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>667</td>\n",
              "      <td>71.580300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>668</td>\n",
              "      <td>187.265600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>669</td>\n",
              "      <td>64.809600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>131.300100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>671</td>\n",
              "      <td>26.167600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>135.961400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>673</td>\n",
              "      <td>10.101200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>674</td>\n",
              "      <td>12.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>9.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>676</td>\n",
              "      <td>113.111300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>677</td>\n",
              "      <td>301.564600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>678</td>\n",
              "      <td>22.979000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>679</td>\n",
              "      <td>132.176700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>198.276400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>681</td>\n",
              "      <td>119.086100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>682</td>\n",
              "      <td>10.856800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>683</td>\n",
              "      <td>10.535200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>28.598700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>685</td>\n",
              "      <td>346.018700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>686</td>\n",
              "      <td>152.537000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>687</td>\n",
              "      <td>108.933900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>688</td>\n",
              "      <td>451.425700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>689</td>\n",
              "      <td>189.243900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>119.306300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>691</td>\n",
              "      <td>225.382500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>692</td>\n",
              "      <td>74.488200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>171.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>694</td>\n",
              "      <td>107.012600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>695</td>\n",
              "      <td>55.949000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>696</td>\n",
              "      <td>194.541900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>697</td>\n",
              "      <td>36.786500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>698</td>\n",
              "      <td>200.273300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>699</td>\n",
              "      <td>167.299500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>75.646900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>701</td>\n",
              "      <td>164.608000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>702</td>\n",
              "      <td>183.766700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>703</td>\n",
              "      <td>74.485100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>704</td>\n",
              "      <td>24.817200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>705</td>\n",
              "      <td>46.314300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>706</td>\n",
              "      <td>46.625100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>707</td>\n",
              "      <td>47.355900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>708</td>\n",
              "      <td>14.672900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>709</td>\n",
              "      <td>185.384000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>169.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>711</td>\n",
              "      <td>215.540300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>712</td>\n",
              "      <td>38.902500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>713</td>\n",
              "      <td>72.963500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>714</td>\n",
              "      <td>143.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>715</td>\n",
              "      <td>128.939600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>716</td>\n",
              "      <td>76.809700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>717</td>\n",
              "      <td>140.554500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>718</td>\n",
              "      <td>66.255100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>719</td>\n",
              "      <td>76.972400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>238.054100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>721</td>\n",
              "      <td>110.546900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>722</td>\n",
              "      <td>81.964700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>723</td>\n",
              "      <td>148.361900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>724</td>\n",
              "      <td>211.690500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>142.717600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>726</td>\n",
              "      <td>31.136700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>727</td>\n",
              "      <td>51.702300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>728</td>\n",
              "      <td>26.185200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>729</td>\n",
              "      <td>204.805400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>93.390900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>731</td>\n",
              "      <td>149.565900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>732</td>\n",
              "      <td>116.720800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>733</td>\n",
              "      <td>12.465800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>734</td>\n",
              "      <td>202.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>25.048400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>736</td>\n",
              "      <td>38.504900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>737</td>\n",
              "      <td>324.417400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>738</td>\n",
              "      <td>757.246500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>739</td>\n",
              "      <td>179.534100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>58.052400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>77.630400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>742</td>\n",
              "      <td>329.716600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>743</td>\n",
              "      <td>39.290700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>744</td>\n",
              "      <td>106.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>745</td>\n",
              "      <td>20.709300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>746</td>\n",
              "      <td>561.239000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>747</td>\n",
              "      <td>35.009500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>33.906900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>749</td>\n",
              "      <td>106.300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>150.939900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>751</td>\n",
              "      <td>55.525000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>752</td>\n",
              "      <td>59.993300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>753</td>\n",
              "      <td>19.650600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>754</td>\n",
              "      <td>26.490100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>755</td>\n",
              "      <td>60.196500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>756</td>\n",
              "      <td>34.562100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>757</td>\n",
              "      <td>65.556300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>758</td>\n",
              "      <td>61.680700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>759</td>\n",
              "      <td>48.410800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>54.034600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>761</td>\n",
              "      <td>51.659300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>762</td>\n",
              "      <td>199.924700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>763</td>\n",
              "      <td>160.486800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>764</td>\n",
              "      <td>82.983200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>765</td>\n",
              "      <td>118.963000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>766</td>\n",
              "      <td>170.685200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>767</td>\n",
              "      <td>58.308300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>768</td>\n",
              "      <td>126.642300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>769</td>\n",
              "      <td>75.487700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>294.334400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>771</td>\n",
              "      <td>100.609800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>772</td>\n",
              "      <td>61.906300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>773</td>\n",
              "      <td>121.582300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>774</td>\n",
              "      <td>100.296000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>182.998400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>776</td>\n",
              "      <td>38.015700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>777</td>\n",
              "      <td>125.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>778</td>\n",
              "      <td>83.926300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>779</td>\n",
              "      <td>88.480900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>122.724500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>781</td>\n",
              "      <td>6.774600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>782</td>\n",
              "      <td>469.273600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>783</td>\n",
              "      <td>30.059800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>784</td>\n",
              "      <td>238.052200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>785</td>\n",
              "      <td>18.543800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>786</td>\n",
              "      <td>77.113100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>787</td>\n",
              "      <td>57.172900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>788</td>\n",
              "      <td>83.144100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>789</td>\n",
              "      <td>174.045900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>6.333700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>791</td>\n",
              "      <td>176.464000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>792</td>\n",
              "      <td>164.505400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>793</td>\n",
              "      <td>97.996300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>794</td>\n",
              "      <td>95.945600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>795</td>\n",
              "      <td>223.819500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>796</td>\n",
              "      <td>59.826300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>797</td>\n",
              "      <td>11.855000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>134.594000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>799</td>\n",
              "      <td>131.655800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>153.181900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>801</td>\n",
              "      <td>30.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>802</td>\n",
              "      <td>42.328500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>803</td>\n",
              "      <td>172.519800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>804</td>\n",
              "      <td>81.819500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>30.623600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>806</td>\n",
              "      <td>17.378300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>807</td>\n",
              "      <td>262.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>808</td>\n",
              "      <td>13.840300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>809</td>\n",
              "      <td>66.987900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>12.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>811</td>\n",
              "      <td>91.593300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>812</td>\n",
              "      <td>599.919400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>813</td>\n",
              "      <td>238.966700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>814</td>\n",
              "      <td>23.307300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>815</td>\n",
              "      <td>116.541500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>816</td>\n",
              "      <td>78.229400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>817</td>\n",
              "      <td>190.864400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>818</td>\n",
              "      <td>72.453900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>819</td>\n",
              "      <td>168.807800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>324.060300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>821</td>\n",
              "      <td>121.020600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>822</td>\n",
              "      <td>276.027400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>823</td>\n",
              "      <td>64.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>824</td>\n",
              "      <td>177.361600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>60.641300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>826</td>\n",
              "      <td>110.936200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>827</td>\n",
              "      <td>164.133400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>828</td>\n",
              "      <td>128.171400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>829</td>\n",
              "      <td>9.392900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>21.335800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>831</td>\n",
              "      <td>171.783500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>832</td>\n",
              "      <td>47.918400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>833</td>\n",
              "      <td>49.911700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>834</td>\n",
              "      <td>90.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>835</td>\n",
              "      <td>216.338600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>836</td>\n",
              "      <td>24.293300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>837</td>\n",
              "      <td>579.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>838</td>\n",
              "      <td>31.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>839</td>\n",
              "      <td>190.043800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>71.937400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>841</td>\n",
              "      <td>55.161700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>842</td>\n",
              "      <td>34.067900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>843</td>\n",
              "      <td>32.077200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>844</td>\n",
              "      <td>423.807300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>845</td>\n",
              "      <td>6.055400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>846</td>\n",
              "      <td>83.364600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>847</td>\n",
              "      <td>123.153800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>848</td>\n",
              "      <td>119.398200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>849</td>\n",
              "      <td>202.437600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>126.685000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>851</td>\n",
              "      <td>49.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>852</td>\n",
              "      <td>20.178800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>853</td>\n",
              "      <td>23.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>854</td>\n",
              "      <td>60.010700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>30.921500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>856</td>\n",
              "      <td>44.324300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>857</td>\n",
              "      <td>52.541300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>858</td>\n",
              "      <td>38.438900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>859</td>\n",
              "      <td>48.873500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>25.253400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>861</td>\n",
              "      <td>106.173100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>862</td>\n",
              "      <td>168.565400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>863</td>\n",
              "      <td>69.308700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>864</td>\n",
              "      <td>64.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>865</td>\n",
              "      <td>268.451200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>866</td>\n",
              "      <td>98.935800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>867</td>\n",
              "      <td>67.780100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>868</td>\n",
              "      <td>117.773100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>869</td>\n",
              "      <td>116.390800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>148.193600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>871</td>\n",
              "      <td>7.027500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>872</td>\n",
              "      <td>58.101200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>873</td>\n",
              "      <td>471.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>874</td>\n",
              "      <td>145.779300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>269.274700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>876</td>\n",
              "      <td>7.546700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>877</td>\n",
              "      <td>150.534500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>878</td>\n",
              "      <td>148.998600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>879</td>\n",
              "      <td>242.398800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>14.254100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>881</td>\n",
              "      <td>132.113400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>882</td>\n",
              "      <td>108.790900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>883</td>\n",
              "      <td>12.777400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>884</td>\n",
              "      <td>27.357100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>885</td>\n",
              "      <td>179.404300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>886</td>\n",
              "      <td>135.483200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>887</td>\n",
              "      <td>185.511800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>888</td>\n",
              "      <td>13.251500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>889</td>\n",
              "      <td>14.274600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>178.364800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>891</td>\n",
              "      <td>44.429000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>892</td>\n",
              "      <td>76.065200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>893</td>\n",
              "      <td>54.710300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>894</td>\n",
              "      <td>94.319900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>895</td>\n",
              "      <td>112.908400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>896</td>\n",
              "      <td>194.249900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>897</td>\n",
              "      <td>26.775300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>898</td>\n",
              "      <td>69.061900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>899</td>\n",
              "      <td>113.707100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>33.953800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>901</td>\n",
              "      <td>233.311300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>902</td>\n",
              "      <td>149.830700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>903</td>\n",
              "      <td>67.558100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>904</td>\n",
              "      <td>117.980700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>905</td>\n",
              "      <td>17.299900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>906</td>\n",
              "      <td>675.794300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>907</td>\n",
              "      <td>36.831200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>908</td>\n",
              "      <td>37.943800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>909</td>\n",
              "      <td>11.042100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>516.733400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>911</td>\n",
              "      <td>72.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>912</td>\n",
              "      <td>124.439400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>913</td>\n",
              "      <td>64.681100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>914</td>\n",
              "      <td>77.676800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>915</td>\n",
              "      <td>132.422100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>916</td>\n",
              "      <td>202.507900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>917</td>\n",
              "      <td>88.721300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>918</td>\n",
              "      <td>119.570100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>919</td>\n",
              "      <td>126.638800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>68.981400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>921</td>\n",
              "      <td>169.513000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>922</td>\n",
              "      <td>97.135700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>923</td>\n",
              "      <td>237.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>91.286600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>11.153300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>926</td>\n",
              "      <td>25.973800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>927</td>\n",
              "      <td>73.983800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>928</td>\n",
              "      <td>178.080400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>929</td>\n",
              "      <td>297.643300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>37.514400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>931</td>\n",
              "      <td>172.169000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>932</td>\n",
              "      <td>326.525700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>933</td>\n",
              "      <td>80.349100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>934</td>\n",
              "      <td>143.166100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>935</td>\n",
              "      <td>63.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>936</td>\n",
              "      <td>38.648400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>937</td>\n",
              "      <td>100.033500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>938</td>\n",
              "      <td>60.507500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>939</td>\n",
              "      <td>5.338200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>178.698400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>941</td>\n",
              "      <td>152.217500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>942</td>\n",
              "      <td>113.523800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>943</td>\n",
              "      <td>129.993400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>944</td>\n",
              "      <td>52.722300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>945</td>\n",
              "      <td>151.299600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>946</td>\n",
              "      <td>38.649700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>947</td>\n",
              "      <td>32.896400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>948</td>\n",
              "      <td>2.294100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>949</td>\n",
              "      <td>43.346700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>82.730600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>951</td>\n",
              "      <td>71.256900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>952</td>\n",
              "      <td>60.483400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>953</td>\n",
              "      <td>300.541700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>954</td>\n",
              "      <td>23.405300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>955</td>\n",
              "      <td>9.213500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>956</td>\n",
              "      <td>114.255300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>957</td>\n",
              "      <td>109.395500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>958</td>\n",
              "      <td>32.784700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>959</td>\n",
              "      <td>146.539400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>26.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>961</td>\n",
              "      <td>166.539600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>962</td>\n",
              "      <td>61.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>963</td>\n",
              "      <td>136.044900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>964</td>\n",
              "      <td>46.309700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>965</td>\n",
              "      <td>239.714200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>966</td>\n",
              "      <td>113.703600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>967</td>\n",
              "      <td>36.069800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>968</td>\n",
              "      <td>47.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>969</td>\n",
              "      <td>388.177800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>28.625600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>971</td>\n",
              "      <td>90.878600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>972</td>\n",
              "      <td>100.955400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>973</td>\n",
              "      <td>609.258900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>974</td>\n",
              "      <td>123.248900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>343.083700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>976</td>\n",
              "      <td>49.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>977</td>\n",
              "      <td>218.334600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>978</td>\n",
              "      <td>98.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>979</td>\n",
              "      <td>101.993300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>58.290200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>981</td>\n",
              "      <td>204.931600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>982</td>\n",
              "      <td>58.314900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>983</td>\n",
              "      <td>76.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>984</td>\n",
              "      <td>27.981400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>985</td>\n",
              "      <td>22.857000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>986</td>\n",
              "      <td>51.621200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>987</td>\n",
              "      <td>262.001700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=987, training_loss=136.84860182412368, metrics={'train_runtime': 1175.9699, 'train_samples_per_second': 3.358, 'train_steps_per_second': 0.839, 'total_flos': 6.384261761160192e+16, 'train_loss': 136.84860182412368})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
        "# from trl import SFTTrainer\n",
        "\n",
        "# # Hàm tính KD loss: sử dụng KL divergence giữa phân phối của teacher và student.\n",
        "# def compute_kd_loss(student_logits, teacher_logits, temperature=2.0):\n",
        "#     # Áp dụng temperature scaling\n",
        "#     student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "#     teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
        "#     # Tính KL divergence (lưu ý nhân với temperature^2 như trong nhiều paper)\n",
        "#     kd_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
        "#     return kd_loss\n",
        "\n",
        "# # Custom trainer kết hợp LM loss và KD loss\n",
        "# class CustomKDTrainer(SFTTrainer):\n",
        "#     def __init__(self, teacher_model, kd_alpha=0.5, kd_temperature=2.0, *args, **kwargs):\n",
        "#         \"\"\"\n",
        "#         teacher_model: Mô hình gốc (teacher) đã được huấn luyện, giữ ở chế độ eval.\n",
        "#         kd_alpha: Hệ số cân bằng giữa LM loss và KD loss. Tổng loss = (1 - kd_alpha) * LM_loss + kd_alpha * KD_loss.\n",
        "#         kd_temperature: Temperature scaling cho KD loss.\n",
        "#         \"\"\"\n",
        "#         super().__init__(*args, **kwargs)\n",
        "#         self.teacher_model = teacher_model\n",
        "#         self.teacher_model.eval()  # Đảm bảo teacher không update\n",
        "#         self.kd_alpha = kd_alpha\n",
        "#         self.kd_temperature = kd_temperature\n",
        "\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
        "#         # Forward pass của student:\n",
        "#         student_outputs = model(\n",
        "#             input_ids=inputs[\"input_ids\"],\n",
        "#             attention_mask=inputs[\"attention_mask\"],\n",
        "#             labels=inputs[\"input_ids\"],  # LM loss: dự đoán token tiếp theo\n",
        "#             return_dict=True,\n",
        "#         )\n",
        "#         student_loss = student_outputs.loss\n",
        "#         student_logits = student_outputs.logits  # shape: (batch, seq_length, vocab_size)\n",
        "\n",
        "#         # Forward pass của teacher (với no_grad để không update teacher)\n",
        "#         with torch.no_grad():\n",
        "#             teacher_outputs = self.teacher_model(\n",
        "#                 input_ids=inputs[\"input_ids\"],\n",
        "#                 attention_mask=inputs[\"attention_mask\"],\n",
        "#                 return_dict=True,\n",
        "#             )\n",
        "#         teacher_logits = teacher_outputs.logits  # shape: (batch, seq_length, vocab_size)\n",
        "\n",
        "#         # Tính KD loss trên toàn batch\n",
        "#         kd_loss = compute_kd_loss(student_logits, teacher_logits, temperature=self.kd_temperature)\n",
        "\n",
        "#         # Tổng hợp loss với trọng số kd_alpha\n",
        "#         total_loss = (1 - self.kd_alpha) * student_loss + self.kd_alpha * kd_loss\n",
        "\n",
        "#         return (total_loss, student_outputs) if return_outputs else total_loss\n",
        "\n",
        "# # Ví dụ sử dụng:\n",
        "# # Giả sử bạn có tên model của student và teacher (ví dụ, cùng một kiến trúc LLaMA)\n",
        "# student_model_name = \"llama-student-model\"\n",
        "# teacher_model_name = \"llama-teacher-model\"  # Mô hình gốc đã được pretrain\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
        "# student_model = AutoModelForCausalLM.from_pretrained(student_model_name)\n",
        "# teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name)\n",
        "\n",
        "# # Training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./kd_finetuned_llama\",\n",
        "#     per_device_train_batch_size=2,\n",
        "#     num_train_epochs=3,\n",
        "#     learning_rate=2e-4,\n",
        "#     logging_steps=10,\n",
        "#     fp16=True,\n",
        "# )\n",
        "\n",
        "# # Giả sử train_dataset của bạn đã được chuẩn bị với các trường \"input_ids\", \"attention_mask\", \"labels\"\n",
        "# # Ví dụ: dataset = load_dataset(...).map(tokenize_function, batched=True)\n",
        "# # Đảm bảo dataset đã set_format để các cột tensor được giữ lại.\n",
        "\n",
        "# trainer = CustomKDTrainer(\n",
        "#     teacher_model=teacher_model,\n",
        "#     model=student_model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     kd_alpha=0.5,         # Điều chỉnh trọng số giữa LM loss và KD loss\n",
        "#     kd_temperature=2.0,   # Temperature scaling cho KD loss\n",
        "# )\n",
        "\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "id": "s59_nAqta_R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdirjw7TLhSy",
        "outputId": "65b873de-f678-462d-c97b-1eee25449c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "13<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "datatest=pd.read_csv(\"/content/drive/MyDrive/CodeMMLU_sample/b6_test_data.csv\")\n",
        "list_task_ids=[]\n",
        "list_answers=[]\n",
        "\n",
        "for index, row in datatest.iterrows():\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            task_instruction, # instruction\n",
        "            str(row[\"question\"]), # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "    list_task_ids.append(str(row[\"task_id\"]))\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "    list_answers.append(tokenizer.batch_decode(outputs)[0][-14])\n",
        "\n",
        "\n",
        "# Unsloth: Input IDs of length 2260 > the model's max sequence length of 2048.\n",
        "# We shall truncate it ourselves. It's imperative if you correct this issue first"
      ],
      "metadata": {
        "id": "_g3DhaQuHiH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_answers_upcase=[]\n",
        "for i in list_answers:\n",
        "    if i not in \"ABCDabcd\":\n",
        "      print (i)\n",
        "      list_answers_upcase.append(\"A\")\n",
        "    else:\n",
        "      list_answers_upcase.append(i.upper())\n",
        "# list_answers_upcase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCEFIkc6HiH5",
        "outputId": "a02b4618-bed9-431c-dc85-e6d21fafb936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('submissioninfer_kd_A.csv', 'w', encoding=\"utf8\", newline='') as myfile:\n",
        "    wr = csv.writer(myfile)\n",
        "    wr.writerow((\"task_id\", \"answer\"))\n",
        "    wr.writerows(zip(list_task_ids,list_answers_upcase))"
      ],
      "metadata": {
        "id": "3YjFUkF-HiH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Major_vote"
      ],
      "metadata": {
        "id": "sms9Kjd71wnU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wBHCUrP07cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "# Giả sử các file có tên dạng \"submission_1.csv\", \"submission_2.csv\", ...\n",
        "# Bạn có thể sử dụng glob để lấy danh sách các file:\n",
        "file_list =[\"/content/submissionmanh1.csv\",\"/content/submissionmanh2.csv\",\"/content/submissioncuoi062.csv\",\"/content/submission_llama054.csv\",\"/content/submission069noise.csv\",\"/content/submission0692dataset.csv\",\"/content/submission064.csv\"]\n",
        "\n",
        "# Khởi tạo dictionary để lưu danh sách câu trả lời cho mỗi task_id\n",
        "answers_dict = {}\n",
        "\n",
        "# Duyệt qua từng file CSV\n",
        "for file in file_list:\n",
        "    df = pd.read_csv(file)\n",
        "    for idx, row in df.iterrows():\n",
        "        task_id = row['task_id']\n",
        "        answer = row['answer']\n",
        "        if task_id not in answers_dict:\n",
        "            answers_dict[task_id] = []\n",
        "        answers_dict[task_id].append(answer)\n",
        "\n",
        "# Tính majority vote cho từng task_id\n",
        "final_data = []\n",
        "for task_id, answers in answers_dict.items():\n",
        "    count = Counter(answers)\n",
        "    # Lấy đáp án có số phiếu cao nhất; nếu tie, Counter.most_common() trả về theo thứ tự gặp đầu tiên.\n",
        "    majority_answer, _ = count.most_common(1)[0]\n",
        "    final_data.append({\"task_id\": task_id, \"answer\": majority_answer})\n",
        "\n",
        "# Chuyển kết quả thành DataFrame và sắp xếp theo task_id (nếu cần)\n",
        "final_df = pd.DataFrame(final_data).sort_values(by=\"task_id\")\n",
        "\n",
        "# Ghi kết quả ra file CSV\n",
        "final_df.to_csv(\"submission_final.csv\", index=False)\n",
        "\n",
        "print(\"Đã tạo file submission_final.csv với majority vote của các đáp án.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGPVYIsXX3Il",
        "outputId": "8fff48e3-a6a6-48ad-9369-2f088e06d7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đã tạo file submission_final.csv với majority vote của các đáp án.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d40dbbb49309486aa13cb0585fc76c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_213b67dd18ee4fac9a612c85648e40de",
              "IPY_MODEL_fae91604b50a40d3be39b54a1dcc0466",
              "IPY_MODEL_1c7d60e7fae0442493fb971f3055a052"
            ],
            "layout": "IPY_MODEL_fd2d7d5ab9b4467e8dfde06825918934"
          }
        },
        "213b67dd18ee4fac9a612c85648e40de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e93be1f4ea4345c58f145d217eb3f492",
            "placeholder": "​",
            "style": "IPY_MODEL_7aa464c3a84f4b04ad946842b78751a6",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "fae91604b50a40d3be39b54a1dcc0466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a17299418c304784b5e2af09e26be578",
            "max": 3963,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2528b60bac146dab20d16a9a87578d4",
            "value": 3963
          }
        },
        "1c7d60e7fae0442493fb971f3055a052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c4c170929324140b041a43ca350e429",
            "placeholder": "​",
            "style": "IPY_MODEL_9438a38fa59b4a4897021d1ecea6aec6",
            "value": " 3963/3963 [00:06&lt;00:00, 749.84 examples/s]"
          }
        },
        "fd2d7d5ab9b4467e8dfde06825918934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e93be1f4ea4345c58f145d217eb3f492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa464c3a84f4b04ad946842b78751a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a17299418c304784b5e2af09e26be578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2528b60bac146dab20d16a9a87578d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c4c170929324140b041a43ca350e429": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9438a38fa59b4a4897021d1ecea6aec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cfd10112c524186b5e6affae6084998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40b57e951743436786fec3f669504732",
              "IPY_MODEL_35593b6060ce4b33ac2ee03b9595f43c",
              "IPY_MODEL_cf8842d289694058a2ff2f2deb7aeec8"
            ],
            "layout": "IPY_MODEL_8c3ca9b29f394a1ab00dde6540b70d4a"
          }
        },
        "40b57e951743436786fec3f669504732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b6d482585c483e92b85d35f60f35c0",
            "placeholder": "​",
            "style": "IPY_MODEL_bbe5b3c74bf044d8a29bd491b22961f0",
            "value": "model.safetensors: 100%"
          }
        },
        "35593b6060ce4b33ac2ee03b9595f43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a226cc7788c42ad8b7f6e6de466fa5c",
            "max": 5964186429,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d5e6833219147c9995bf24b351308bb",
            "value": 5964185861
          }
        },
        "cf8842d289694058a2ff2f2deb7aeec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c40d3f5e077148d1ad2d6b66df096298",
            "placeholder": "​",
            "style": "IPY_MODEL_eab46d56a03f435d931ead9448820357",
            "value": " 5.96G/5.96G [00:47&lt;00:00, 427MB/s]"
          }
        },
        "8c3ca9b29f394a1ab00dde6540b70d4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50b6d482585c483e92b85d35f60f35c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe5b3c74bf044d8a29bd491b22961f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a226cc7788c42ad8b7f6e6de466fa5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d5e6833219147c9995bf24b351308bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c40d3f5e077148d1ad2d6b66df096298": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eab46d56a03f435d931ead9448820357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79fb284ac359436abd83556f61081320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b34592c34154477b03a144e2f86beaa",
              "IPY_MODEL_cab5b5c531e3482aa606fe5ce47976aa",
              "IPY_MODEL_b5f33998d8164dbe966d48a4e4c3b2d3"
            ],
            "layout": "IPY_MODEL_2dc083de20ad4043a8eb4e1e9c861d54"
          }
        },
        "0b34592c34154477b03a144e2f86beaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a56983934b4f3d95bd7411d851bf64",
            "placeholder": "​",
            "style": "IPY_MODEL_a5f69e8f53aa4e5fb0cde6cbcd2b5a1f",
            "value": "generation_config.json: 100%"
          }
        },
        "cab5b5c531e3482aa606fe5ce47976aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f17c70b77824645822d02298929b308",
            "max": 235,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be5688fce75a41c1b4652e8435c35c7d",
            "value": 235
          }
        },
        "b5f33998d8164dbe966d48a4e4c3b2d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_420daf1a24a241e38bea251d3dc5d76f",
            "placeholder": "​",
            "style": "IPY_MODEL_be1fdc4da45f45aebaa4b39275b5f83f",
            "value": " 235/235 [00:00&lt;00:00, 8.32kB/s]"
          }
        },
        "2dc083de20ad4043a8eb4e1e9c861d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a56983934b4f3d95bd7411d851bf64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f69e8f53aa4e5fb0cde6cbcd2b5a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f17c70b77824645822d02298929b308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be5688fce75a41c1b4652e8435c35c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "420daf1a24a241e38bea251d3dc5d76f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be1fdc4da45f45aebaa4b39275b5f83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a235deb6b4ad41469d71adb663bdc75d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5dbc5190c5c4307862ec5b8e95d3b2d",
              "IPY_MODEL_f6196cdb1f294c23867be2e0c3491c7b",
              "IPY_MODEL_e31df3d3be254b669b48f4f40d9128a2"
            ],
            "layout": "IPY_MODEL_b728a04e209448778ad56d60f22c280f"
          }
        },
        "a5dbc5190c5c4307862ec5b8e95d3b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7263272c3f84995bf1ead854136c088",
            "placeholder": "​",
            "style": "IPY_MODEL_4debddb59c9946d497ff562242abf389",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f6196cdb1f294c23867be2e0c3491c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe1493c5a6424e9d8228e987873e8df7",
            "max": 50642,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7aa12b27fdcd4e038dd98ab82f1dbdc7",
            "value": 50642
          }
        },
        "e31df3d3be254b669b48f4f40d9128a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f49eb9c4c647628b184a1d5bf81102",
            "placeholder": "​",
            "style": "IPY_MODEL_4864216bccf74e9aafbfef27c8e93ff2",
            "value": " 50.6k/50.6k [00:00&lt;00:00, 2.71MB/s]"
          }
        },
        "b728a04e209448778ad56d60f22c280f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7263272c3f84995bf1ead854136c088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4debddb59c9946d497ff562242abf389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe1493c5a6424e9d8228e987873e8df7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa12b27fdcd4e038dd98ab82f1dbdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0f49eb9c4c647628b184a1d5bf81102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4864216bccf74e9aafbfef27c8e93ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d7793b5e7dc4403837b7c9b762e9811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4fc1e24c2b14eaea946ae5c83a547bd",
              "IPY_MODEL_2985d6bd22f34d1fa7b2df3a80e26f65",
              "IPY_MODEL_26c134aaa07b41008094c3f5fc99860d"
            ],
            "layout": "IPY_MODEL_8fc7c654f1054fec9067b6f9ae9dbaab"
          }
        },
        "a4fc1e24c2b14eaea946ae5c83a547bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7046b49bd5f340bc926a93ec4068a9f3",
            "placeholder": "​",
            "style": "IPY_MODEL_0894698255114f518385aabb9b85cceb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "2985d6bd22f34d1fa7b2df3a80e26f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03524fe50f5348b6b886a88910e40cd2",
            "max": 459,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac87a7dd9fcd48e498578124705164f8",
            "value": 459
          }
        },
        "26c134aaa07b41008094c3f5fc99860d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05cdb9c71dd34e3ba261a1bf8540dc90",
            "placeholder": "​",
            "style": "IPY_MODEL_64f280fa3f364a148f27be633d4e8602",
            "value": " 459/459 [00:00&lt;00:00, 12.1kB/s]"
          }
        },
        "8fc7c654f1054fec9067b6f9ae9dbaab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7046b49bd5f340bc926a93ec4068a9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0894698255114f518385aabb9b85cceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03524fe50f5348b6b886a88910e40cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac87a7dd9fcd48e498578124705164f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05cdb9c71dd34e3ba261a1bf8540dc90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64f280fa3f364a148f27be633d4e8602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "261e4ff5645a4f1ca809b5dde7629560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_009a5e12304c4168af679fa6a991fe1f",
              "IPY_MODEL_00597430a30d4015ba7453de03b0c352",
              "IPY_MODEL_dd879f3b989f4a45874e392b6503bdcc"
            ],
            "layout": "IPY_MODEL_2c77356ce1724002ade402c363952527"
          }
        },
        "009a5e12304c4168af679fa6a991fe1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_713aba8a3041420c9ee310b4ddb75d0a",
            "placeholder": "​",
            "style": "IPY_MODEL_4194e22e837e4c48a34b0c257452c4e5",
            "value": "tokenizer.json: 100%"
          }
        },
        "00597430a30d4015ba7453de03b0c352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb71ff03f13f41beb95da131268243be",
            "max": 17209920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_019937b69fd7426a992d4a666d7ac6cb",
            "value": 17209920
          }
        },
        "dd879f3b989f4a45874e392b6503bdcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ed38353eafa4ef3a909de51eea4081e",
            "placeholder": "​",
            "style": "IPY_MODEL_f40997ca82d74070b63668495fcae48f",
            "value": " 17.2M/17.2M [00:00&lt;00:00, 28.8MB/s]"
          }
        },
        "2c77356ce1724002ade402c363952527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "713aba8a3041420c9ee310b4ddb75d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4194e22e837e4c48a34b0c257452c4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb71ff03f13f41beb95da131268243be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "019937b69fd7426a992d4a666d7ac6cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ed38353eafa4ef3a909de51eea4081e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f40997ca82d74070b63668495fcae48f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f53f35fecaba4d1f93d7e274917f07b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fde00f495216423b9e534ed5ba19259f",
              "IPY_MODEL_e312c78649574f9bbbaa16d8e4a5f5ad",
              "IPY_MODEL_766c595537064cbbb0cdb8188fc1e921"
            ],
            "layout": "IPY_MODEL_6d1a26d7d559462196c340269d10c1a1"
          }
        },
        "fde00f495216423b9e534ed5ba19259f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7717926adfb14c5583bf3a1b5deb9cfc",
            "placeholder": "​",
            "style": "IPY_MODEL_53a3999637f54beeab4c9a90d72797cc",
            "value": "Map: 100%"
          }
        },
        "e312c78649574f9bbbaa16d8e4a5f5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c7ac5002cfe4bf782b2b205ab989bfd",
            "max": 3949,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3682571366346f1b64933e6edbc7773",
            "value": 3949
          }
        },
        "766c595537064cbbb0cdb8188fc1e921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39403b56f6784e2db79d6144dc2eeb53",
            "placeholder": "​",
            "style": "IPY_MODEL_fbe6c80b65ae4681946667d0ed5690b8",
            "value": " 3949/3949 [00:00&lt;00:00, 9545.00 examples/s]"
          }
        },
        "6d1a26d7d559462196c340269d10c1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7717926adfb14c5583bf3a1b5deb9cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53a3999637f54beeab4c9a90d72797cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c7ac5002cfe4bf782b2b205ab989bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3682571366346f1b64933e6edbc7773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39403b56f6784e2db79d6144dc2eeb53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe6c80b65ae4681946667d0ed5690b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d8749855f0e4383a4ae7fc8c0393992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e8a4c58c181496aadacda715ab8c4dd",
              "IPY_MODEL_9df7309d6a8444588732d419ac9fdd69",
              "IPY_MODEL_b9d10e5dce074081932b1c67986e551f"
            ],
            "layout": "IPY_MODEL_23101f767e2244aebb8a16fca4f9d744"
          }
        },
        "0e8a4c58c181496aadacda715ab8c4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_425b55147263401498412119de507b2e",
            "placeholder": "​",
            "style": "IPY_MODEL_8ca762aa59154886ac178f5174cae605",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "9df7309d6a8444588732d419ac9fdd69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9606651acd2149cdb87e664176d13a18",
            "max": 3949,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bfdda8a3e474516b7a5843571c65819",
            "value": 3949
          }
        },
        "b9d10e5dce074081932b1c67986e551f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebf084a63a824cc5bdee0928f5fc6f21",
            "placeholder": "​",
            "style": "IPY_MODEL_3e67e9c0fc0f44b295f4ffc9fb2cb908",
            "value": " 3949/3949 [00:10&lt;00:00, 450.00 examples/s]"
          }
        },
        "23101f767e2244aebb8a16fca4f9d744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425b55147263401498412119de507b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca762aa59154886ac178f5174cae605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9606651acd2149cdb87e664176d13a18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bfdda8a3e474516b7a5843571c65819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebf084a63a824cc5bdee0928f5fc6f21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e67e9c0fc0f44b295f4ffc9fb2cb908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}