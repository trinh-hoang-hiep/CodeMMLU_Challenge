# Top 8 CodeMMLU_Challenge

About
[ICLR 2025] ðŸš€ CodeMMLU Evaluator: A framework for evaluating LM models on CodeMMLU MCQs benchmark.




CodeMMLU Challenge! This competition evaluates models on multiple-choice programming-related questions using the CodeMMLU dataset. Your task is to develop an AI model that can answer coding-related multiple-choice questions as accurately as possible.

https://www.kaggle.com/competitions/fpt-ai-residency-batch-6-entry-test/overview

ðŸ“Œ Competition Overview
Dataset: CodeMMLU
Task: Predict the correct answer for each multiple-choice question in the dataset.
Evaluation Metric: Accuracy on the test set.

#	Model	Size (B)	Syntactic Acc	Semantic Acc	Fundamental-task Acc	CodeMMLU
1
ðŸ¥‡
GPT 4o
-	60.41	57.82	77.18	67

2
ðŸ¥ˆ
GPT o3-mini
-	53.08	75.5	62.77	62.36

3
ðŸ¥‰
Claude 3.7 Sonnet
-	52.78	76.26	60.92	61.65

4
Meta Llama3.1 70B Instruct
70	64.41	62.25	56.11	60

5
Claude 3.5 Sonnet
-	52.23	73.45	58.56	59.81

6
Meta Llama3.1 405B Inst
405	50.82	71.41	57.1	58.23

7
Cluade 3.5 Haiku
-	49.24	68.2	57.83	57.25

8
QwenCoder2.5 32B Inst
32	50.63	69.61	53.89	56.4

9
Claude 3 Sonnet
-	67.22	66.08	38.26	53.97

![Alt text](/data/Top8CodeMMLU.png)

